{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2fca83",
   "metadata": {},
   "source": [
    "## Evaluate Pydantic AI weather agent\n",
    "This tutorial will show you how to evaluate Pydantic AI agents using DeepEval's dataset iterator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae4769",
   "metadata": {},
   "source": [
    "### Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cd4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic-ai -U deepeval --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90569a",
   "metadata": {},
   "source": [
    "### Set your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac517022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb85503",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Hyperparameters of an LLM are the parameters that are used to control the behavior of the LLM application. It can be model, temperature, max tokens, or even you static prompts (for eg, system prompt). One of the main aim of performing evlauation is to find the best set of hyperparameters for a given agent.\n",
    "\n",
    "For this application, we are using model as one of the hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95070436",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d91c8",
   "metadata": {},
   "source": [
    "### Create a Pydantic AI agent. \n",
    "\n",
    "This is the same example as the one in the [Pydantic AI docs](https://ai.pydantic.dev/examples/weather-agent/). User can ask for the weather in multiple cities, the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\n",
    "the `get_weather` tool to get the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc9177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The weather in London is stormy with a temperature of 11 °C, and in Wiltshire, it's also stormy with a temperature of 13 °C.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "from httpx import AsyncClient\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    client: AsyncClient\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    hyperparameter_model,\n",
    "    instructions='Be concise, reply with one sentence.',\n",
    "    deps_type=Deps,\n",
    "    retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "class LatLng(BaseModel):\n",
    "    lat: float\n",
    "    lng: float\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n",
    "    \"\"\"Get the latitude and longitude of a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        location_description: A description of a location.\n",
    "    \"\"\"\n",
    "    # NOTE: the response here will be random, and is not related to the location description.\n",
    "    r = await ctx.deps.client.get(\n",
    "        'https://demo-endpoints.pydantic.workers.dev/latlng',\n",
    "        params={'location': location_description},\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return LatLng.model_validate_json(r.content)\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
    "    \"\"\"Get the weather at a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        lat: Latitude of the location.\n",
    "        lng: Longitude of the location.\n",
    "    \"\"\"\n",
    "    # NOTE: the responses here will be random, and are not related to the lat and lng.\n",
    "    temp_response, descr_response = await asyncio.gather(\n",
    "        ctx.deps.client.get(\n",
    "            'https://demo-endpoints.pydantic.workers.dev/number',\n",
    "            params={'min': 10, 'max': 30},\n",
    "        ),\n",
    "        ctx.deps.client.get(\n",
    "            'https://demo-endpoints.pydantic.workers.dev/weather',\n",
    "            params={'lat': lat, 'lng': lng},\n",
    "        ),\n",
    "    )\n",
    "    temp_response.raise_for_status()\n",
    "    descr_response.raise_for_status()\n",
    "    return {\n",
    "        'temperature': f'{temp_response.text} °C',\n",
    "        'description': descr_response.text,\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_agent(input_query: str):\n",
    "    async with AsyncClient() as client:\n",
    "        deps = Deps(client=client)\n",
    "        result = await weather_agent.run(\n",
    "            input_query, deps=deps\n",
    "        )\n",
    "        return result.output\n",
    "\n",
    "await run_agent(\"What is the weather like in London and in Wiltshire?\")  # test run the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157564e3",
   "metadata": {},
   "source": [
    "### Evaluate the agent\n",
    "\n",
    "To evaluate Pydantic AI agents:\n",
    "\n",
    "1. Instrument the application (using `from deepeval.integrations.pydantic_ai import instrument_pydantic_ai`)\n",
    "2. Use Deepeval's Pydantic AI `Agent` to supply metrics.\n",
    "\n",
    "\n",
    "> (Pro Tip) View your Agent's trace and publish test runs on [Confident AI](https://www.confident-ai.com/). Apart from this you get an in-house dataset editor and more advaced tools to monitor and enventually improve your Agent's performance. Get your API key from [here](https://app.confident-ai.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CONFIDENT_API_KEY=your-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4bc68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "from deepeval.integrations.pydantic_ai import instrument_pydantic_ai\n",
    "instrument_pydantic_ai()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b5044",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For evaluating the agent, we need a dataset. You can create your own dataset or use the one from the [Confident AI](https://www.confident-ai.com/docs/llm-evaluation/dataset-management/create-goldens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6476a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9255876ea5994d8097a2cb3a773d9ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.pull(alias=\"weather_agent_queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b28c1f",
   "metadata": {},
   "source": [
    "### Create a metric to evaluate the agent.\n",
    "\n",
    "Deepeval provides a state of the art ready to use [metric](https://deepeval.com/docs/metrics-introduction) to evaluate the agent. For this example, we will use the `AnswerRelevancyMetric`.\n",
    "\n",
    "> [!NOTE]\n",
    "You can only run end-to-end evals on metrics that evaluate the input and actual output of your Pydantic agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b395c",
   "metadata": {},
   "source": [
    "Using Deepeval's Pydantic AI `Agent` wrapper, you can supply metrics to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d4f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing agent\n",
      "patching run method\n"
     ]
    }
   ],
   "source": [
    "from deepeval.integrations.pydantic_ai import Agent\n",
    "from deepeval.metrics import BaseMetric\n",
    "\n",
    "weather_agent = Agent(\n",
    "    hyperparameter_model,\n",
    "    instructions='Be concise, reply with one sentence.',\n",
    "    deps_type=Deps,\n",
    "    retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "class LatLng(BaseModel):\n",
    "    lat: float\n",
    "    lng: float\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n",
    "    r = await ctx.deps.client.get('https://demo-endpoints.pydantic.workers.dev/latlng',params={'location': location_description},)\n",
    "    r.raise_for_status()\n",
    "    return LatLng.model_validate_json(r.content)\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
    "\n",
    "    temp_response, descr_response = await asyncio.gather(\n",
    "        ctx.deps.client.get('https://demo-endpoints.pydantic.workers.dev/number',params={'min': 10, 'max': 30},),\n",
    "        ctx.deps.client.get('https://demo-endpoints.pydantic.workers.dev/weather',params={'lat': lat, 'lng': lng},),\n",
    "    )\n",
    "    temp_response.raise_for_status()\n",
    "    descr_response.raise_for_status()\n",
    "    return {\n",
    "        'temperature': f'{temp_response.text} °C',\n",
    "        'description': descr_response.text,\n",
    "    }\n",
    "\n",
    "async def run_agent(input_query: str, metrics: list[BaseMetric]):\n",
    "    async with AsyncClient() as client:\n",
    "        deps = Deps(client=client)\n",
    "        result = await weather_agent.run(\n",
    "            input_query, deps=deps, metrics=metrics\n",
    "        )\n",
    "        return result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5c34f",
   "metadata": {},
   "source": [
    "### Use the dataset iterator to evaluate the agent.\n",
    "\n",
    "Use the dataset iterator (from the dataset that was pulled earlier from the Confident AI) to evaluate the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6631c095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ed58db8ae84b7c9ae830feae3cd993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in Paris and Lyon without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Paris and in Lyon?\n",
      "  - actual output: Paris is currently cloudy with a temperature of 12 °C, and Lyon is experiencing rain with a temperature of 27 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in both Berlin and Hamburg without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Berlin and in Hamburg?\n",
      "  - actual output: Berlin is 11 °C and foggy, while Hamburg is 28 °C and windy.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in New York and Boston without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in New York and in Boston?\n",
      "  - actual output: In New York, it's snowing with a temperature of 25 °C, while in Boston, it's sunny with a temperature of 27 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in both Tokyo and Kyoto without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Tokyo and in Kyoto?\n",
      "  - actual output: The weather in Tokyo is 20 °C and drizzling, while in Kyoto, it is 22 °C and hailing.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in both Toronto and Vancouver without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Toronto and in Vancouver?\n",
      "  - actual output: In Toronto, the weather is foggy with a temperature of 20 °C, while in Vancouver, it's partly cloudy with a temperature of 20 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in both San Francisco and Los Angeles without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in San Francisco and in Los Angeles?\n",
      "  - actual output: San Francisco is currently windy with a temperature of 18 °C, while Los Angeles is experiencing hailing with a temperature of 28 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in Rome and Naples without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Rome and in Naples?\n",
      "  - actual output: In Rome, it's raining with a temperature of 11 °C, and in Naples, it's hailing with a temperature of 28 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in Sydney and Melbourne without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Sydney and in Melbourne?\n",
      "  - actual output: The weather in Sydney is currently raining with a temperature of 19 °C, while in Melbourne it is cloudy with a temperature of 28 °C.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.75, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 0.75 because the answer accurately addresses the weather in Delhi and Mumbai, but includes an irrelevant statement about snow in Mumbai, which is unlikely and detracts from the overall relevancy., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Delhi and in Mumbai?\n",
      "  - actual output: In Delhi, it's 30 °C and partly cloudy, while in Mumbai, it's 21 °C and snowing.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about the weather in both Dubai and Abu Dhabi without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather like in Dubai and in Abu Dhabi?\n",
      "  - actual output: The current weather in Dubai is 14 °C and partly cloudy, while in Abu Dhabi it is 10 °C and windy.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Done 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Done 🎉! View results on \n",
       "\u001b]8;id=733599;https://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=733599;https://app.confident-ai.com/project/cmb80cbq000ot94w15duw9a91/evaluation/test-runs/cmezz0nqm00y23dhh3e4g2v5i/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for golden in dataset.evals_iterator():\n",
    "    task = asyncio.create_task(run_agent(\n",
    "        golden.input,\n",
    "        metrics=[answer_relevancy],\n",
    "    ))\n",
    "    dataset.evaluate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf15ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
