---
id: tutorial-summarization-evaluation
title: Evaluation
sidebar_label: Evaluation
---

In the previous section, we built a meeting summarization agent and reviewed the output it generated from a sample conversation transcript.
But how do we assess the quality of that output? Many developers tend to eyeball the results of their LLM applications, this is a common and significant issue in LLM application development.

In this section we are going to see how to evaluate our `MeetingSummarizer` using **DeepEval**, a powerful open-source LLM evaluation framework.

## Defining Evaluation Criteria

Defining evaluation criteria is arguably the most important part of assessing an LLM application's performance. LLM applications are always made with a clear goal in mind, and the evaluation criteria must be defined by taking this goal into consideration. 
Let's look at the goal we've defined for our meeting summarization agent:

The agent we've defined must process meeting-style transcripts and generate:

- A brief summary of the discussion.
- A list of action items.

Our evaluation criteria are directly based on the two goals mentioned above. Our summarization agent is designed to improve team productivity, and hence the **summaries that are generated must be concise**.
Another key criterion to consider is that the **action items generated must be correct and cover all the key actions** that were mentioned in the meeting. 

## Selecting the metrics

As mentioned above, the criteria we've defined are:

- Generated summaries must be concise
- Generated action items must be correct and cover all key actions.

These two criteria are specific to our `MeetingSummarizer`, but your use case might differ — and that's perfectly fine, `deepeval` supports custom evaluation through the [`GEval`](https://deepeval.com/docs/metrics-llm-evals) metric, which we'll use here. `GEval` lets you define custom criteria for your specific use case. [Here's a great article explaining how G-Eval works](https://www.confident-ai.com/blog/g-eval-the-definitive-guide).

## Running Evaluations

Here's how we can define our metrics and evaluate our `MeetingSummarizer`:

```python title="test_summarizer.py"
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval import evaluate
from meeting_summarizer import MeetingSummarizer # import your summarizer here

summarizer = MeetingSummarizer()
with open("meeting_transcript.txt", "r") as file:
    transcript = file.read().strip()

summary, action_items = summarizer.summarize(transcript)

summary_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=summary # provide the summary generated by your summarizer here
)

action_item_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=str(action_items) # provide the action items generated by your summarizer here
)

summary_concision = GEval(
    name="Summary Concision",
    criteria="Assess whether the summary is concise and focused only on the essential points of the meeting? It should avoid repetition, irrelevant details, and unnecessary elaboration.",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)
action_item_check = GEval(
    name="Action Item Accuracy",
    criteria="Are the action items accurate, complete, and clearly reflect the key tasks or follow-ups mentioned in the meeting?",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)

evaluate(test_cases=[summary_test_case], metrics=[summary_concision])
evaluate(test_cases=[action_item_test_case], metrics=[action_item_check])
```

You can save the above code in a test file named `test_summarizer.py` and run the following code in your terminal to evaluate your summarizer:

```bash
deepeval test run test_summarizer.py
```

`deepeval` now prints your evaluation results in the console.

:::tip
It is highly recommended that you use [**Confident AI**](https://www.confident-ai.com), `deepeval`'s cloud platform that allows you to view your test results in a much more intuitive way. Here's how you can [set up Confident AI](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai). Or you can simply run the following code in the terminal to set it up yourself:
```bash
deepeval login
```
**It's free to get started!** _(No credit card required.)_
:::

That's all it takes to evaluate your summarization agent using `deepeval`. 

## Evaluation Results

Here are the results I got after running the evaluation code:

| Metric                    | Score | Result |
|---------------------------|-------|--------|
| Summary Concision         | 0.7   | Fail   |
| Action Item Accuracy      | 0.8   | Fail   |

**DeepEval**'s metrics provide a reason for their evaluation of a test case, which allows you to debug your LLM application easily on why certain test cases pass or fail. Below are the reasons provided by `deepeval`'s `GEval` for the above evaluation result:

**For summary:**

The summary generated by our summarization agent in the previous section is given below: 

:::tip Summary
**Meeting Summary:**

Ethan and Maya discussed recent feedback on the customer support assistant, focusing on concerns around response speed and answer quality. Key issues included vague or incorrect answers and misclassification of simple issues, which may stem from inaccurate internal summarization.

They debated whether the problems are due to prompt engineering or the model itself. Maya shared results comparing GPT-4o and Claude 3, noting that Claude gave more reliable responses but was slower. Ethan emphasized the importance of latency for user experience.

They considered a hybrid approach using GPT-4o for speed and Claude as a fallback when confidence is low. However, current systems lack effective confidence metrics. They explored using embedding similarity as a potential signal, while being mindful of associated costs.

The conversation also touched on user feedback about the assistant’s robotic tone. Maya recommended prompt tuning with example replies instead of full model fine-tuning to improve tone and empathy.

Finally, they discussed UI strategies for low-confidence responses, agreeing that a fallback prompt suggesting human assistance would improve user trust, provided it's used judiciously.
:::

```text
The Actual Output effectively identifies the key points of the meeting, covering the 
issues with the assistant's performance, the comparison between GPT-4o and Claude 3, 
the proposed hybrid approach, and the discussion around confidence metrics and tone. 
It omits extraneous details and is significantly shorter than the Input transcript. 
There's minimal repetition. However, while concise, it could be *slightly* more 
reduced; some phrasing feels unnecessarily verbose for a summary 
(e.g., 'Ethan and Maya discussed... focusing on concerns').
```

This feedback is fair — the summary is solid but can be trimmed.

**For action items:**

The action items generated by our summarization agent were: 
:::tip Action items
```json
{
  "individual_actions": {
    "Ethan": ["Sync with design on the fallback UX messaging"],
    "Maya": [
      "Build the similarity metric",
      "Set up a test run for the hybrid model approach using GPT-4o and Claude"
    ]
  },
  "team_actions": [],
  "entities": ["Ethan", "Maya"]
}
```
:::

```text
The Actual Output captures some key action items discussed in the Input, specifically 
Maya building the similarity metric and setting up the hybrid model test, and Ethan 
syncing with design. However, it misses several follow-ups, such as exploring 8-bit 
embedding quantization and addressing the robotic tone of the assistant via prompt 
tuning. While the listed actions are clear and accurate, the completeness is lacking. 
The action items directly correspond to tasks mentioned, but not all tasks are 
represented.
```

As the reason states, the action items here indeed look too vague and do not capture all the actions stated in the conversation.

These reasons explain why the test cases failed, and help us identify exactly what needs to be fixed.

:::info
It is advised to use a good evaluation model for better results and reasons. Your evaluation model needs to fit the task it's trying to evaluate. 
Some models like `gpt-4`, `gpt-4o`, `gpt-3.5-turbo` and `claude-3-opus` are best for summarization evaluations.
:::

From the reasons stated by the evaluation model, we can conclude the following:

- The summary needs to be shorter and tighter.
- The action items must include all key tasks.

By taking these reasons into consideration, in the next section we are going to see [how to improve your summarization agent](tutorial-summarization-improvement).