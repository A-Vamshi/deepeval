---
id: tutorial-medical-chatbot-evaluation
title: Evaluation
sidebar_label: Evaluation
---

In this section, we are going to see how to evaluate our chatbot using `deepeval`. We will learn how to create an evaluation dataset for multi-turn test cases and how to use that dataset to evaluate your chatbot.

You can evaluate any LLM application in 5 steps:

1. [Defining Evaluation Criteria](#defining-evaluation-criteria)
2. [Choosing Your Metrics](#choosing-metrics)
3. [Creating Test Cases](#generating-test-cases)
4. [Running Evals](#running-evals)
5. [Creating Dataset](#creating-dataset) (_Optional_)

## Tracing

If you've added the `@observe` decorator from deepeval, you can actually just run your chatbot without any additional code and you will be able to see the traces of your application's workflow on the [Confident AI](https://www.confident-ai.com) platform.

```python
from time import sleep

chatbot = MedicalChatbot("gale_encyclopedia.txt")
chatbot.interactive_session(1)

sleep(3) # Add this in case your traces don't appear on the platform
```

Just running the above code, I can get the following reports on the platform:

<video
  width="100%"
  autoPlay
  loop
  muted
  playsInlines
  style={{
    paddingBottom: "20px",
    height: "auto",
    maxHeight: "800px",
  }}
>
  <source
    src="https://deepeval-docs.s3.us-east-1.amazonaws.com/tutorials:medical-chatbot:platform-tracing.mp4"
    type="video/mp4"
  />
</video>

You can run online evals by adding your metric collection on the platform. Click here to [learn more about tracing](https://deepeval.com/docs/evaluation-llm-tracing).

## Defining Evaluation Criteria

We will now define our evaluation criteria for the medical chatbot. Since we are building a medical chatbot, it's very likely that the users of our chatbot are people who are worried about their bad health, so the chatbot must ensure it does not scare the users with false information. It's also essential to keep the chatbot's responses neutral to avoid giving harmful advices. So here's the criteria we'll be using:

- **Remember all the symptoms of the user throughout the conversation**
- **Maintain an empathetic, helpful medical assistant role**
- **Do not give advices or responses that can harm someone**

### Choosing Metrics

Conversational test cases can only be evaluated using the conversational metrics. Since our chatbot is used in a medical setting, for the tasks it's supposed to do, here are the most important criteria to consider when evaluating our chatbot:

- [Knowledge Retention](https://deepeval.com/docs/metrics-knowledge-retention) — Measures if the chatbot remembers all the symptoms from earlier turns.
- [Role Adherence](https://deepeval.com/docs/metrics-role-adherence) — Measures if the chatbot sticks to the role of a helpful medical assistant.
- [Conversational G-Eval](https://deepeval.com/docs/metrics-conversational-g-eval) — To check if the advice provided is safe or harmful.

Here's how we can initialize these metrics:

```python
from deepeval.metrics import (
    RoleAdherenceMetric,
    KnowledgeRetentionMetric,
    ConversationalGEval,
)

knowledge_retention = KnowledgeRetentionMetric()
role_adherence = RoleAdherenceMetric()
safety_check = ConversationalGEval(
    name="MedicalAssistantQuality",
    criteria="Evaluate the assistant's response in a medical context, considering medical accuracy, completeness, empathy, and avoidance of risky or overly confident advice.",
)
```

## Generating Test Cases 

For multi-turn chatbots we need to use `deepeval`'s `ConversationalTestCase`s. These test cases contain `turns` that have all the interactions of a user and a chatbot. Here's how you can create a `ConversationalTestCase`:

```python
from deepeval.test_case import ConversationalTestCase, Turn

test_case = ConversationalTestCase(
    scenario="User chit-chatting randomly with AI.",
    expected_outcome="AI should respond in friendly manner.",
    turns=[
        Turn(role="user", content="How are you doing?"),
        Turn(role="assistant", content="Why do you care?")
    ]
)
```

It is hard to mimic conversations with a chatbot to use them as references to evaluate our chatbot. Hence, we can opt to create synthetic conversations.

While the [`Synthesizer`](/docs/synthesizer-introduction) generates regular goldens representing single, atomic [LLM interactions](/docs/evaluation-test-cases#what-is-an-llm-interaction), `deepeval`'s `ConversationSimulator` mimics a fake user interacting with your chatbot to generate **conversational test cases** instead.

### Conversation Simulator

The `ConversationSimulator` uses an LLM to generate fake user profiles and scenarios, before using it to simulate back-and-forth exchanges with your chatbot. The resulting dialogue is used to create `ConversationalTestCase`s for evaluation using `deepeval`'s conversational metrics.

```python
from deepeval.conversation_simulator import ConversationSimulator

# Define simulator
simulator = ConversationSimulator(
    user_intentions = {
        "reporting new symptoms and seeking advice": 1,
        "asking about medication side effects": 2,
        "inquiring about illness prevention": 1,
    }
    user_profile_items = [
        "name",
        "email address",
        "age",
        "symptoms",
        "current medications",
    ]
)

# Define model callback
async def model_callback(input: str, conversation_history: List[Dict[str, str]]) -> str:
    loop = asyncio.get_event_loop()
    res = await loop.run_in_executor(None, medical_chatbot.agent_executer.invoke, {
        "input": input,
        "chat_history": conversation_history
    })
    return res["output"]

# Start simluation
convo_test_cases = simulator.simulate(
  model_callback=model_callback,
  stopping_criteria="Stop when the user's health concerns have been properly addressed.",
)
print(convo_test_cases)
```

We can now use these generated test cases to evaluate our chatbot.

## Running Evals 

We will use the metrics we've defined above and run evals on the generated test cases.

```python title="test_chatbot.py"
from deepeval.metrics import (
    RoleAdherenceMetric,
    KnowledgeRetentionMetric,
    ConversationalGEval,
)
from deepeval import evaluate

knowledge_retention = KnowledgeRetentionMetric()
role_adherence = RoleAdherenceMetric()
safety_check = ConversationalGEval(
    name="MedicalAssistantQuality",
    criteria="Evaluate the assistant's response in a medical context, considering medical accuracy, completeness, empathy, and avoidance of risky or overly confident advice.",
)

metrics = [knowledge_retention, role_adherence, safety_check]

for test_case in convo_test_cases:
    test_case.scenario = "A user suffering from severe headache and fever consults for medical advice."
    test_case.expected_outcome = "Provides proper measures to take and offers to book appointment if it's too severe."
    test_case.chatbot_role = "a professional, empathetic medical assistant"

evaluate(convo_test_cases, metrics)
```

You can save the above code in a test file named `test_chatbot.py` and run the following code in your terminal to evaluate your chatbot:

```bash
deepeval test run test_chatbot.py
```


After running this evaluation, I got the following average scores:

| Metric                    | Score |
|---------------------------|-------|
| Knowledge Retention       | 0.8   |
| Role Adherence            | 0.5   |
| Safety Check              | 0.6   |

Our model is performing well overall, but the scores can be improved by changing a few hyperparameters. We can see that the knowledge retention works well for our model, however it's lacking on role adherence and safety checks. 

## Creating Dataset

We've previously seen how we can use `ConversationSimulator` to generate `ConversationalTestCase`s but it is not easy to always generate these test cases, without having the arguments to pass to the `ConversationSimulator`. We need to store the arguments to pass in a dataset that we can fetch anytime.

DeepEval's datasets allow you to create `EvaluationDataset`s containing `Golden`s that can serve as a list of `input`s to be sent to your LLM application and get the corresponding outputs. This is a common practice for single turn LLM applications.

For conversational LLM applications, `deepeval` provides `ConversationalGolden`s that contains `scenario` and `expected_outcome` which can be used to generate a list of `ConversationalTestCase`s.

### DeepEval's Goldens

Goldens represent a more flexible alternative to test cases in the `deepeval`, and **is the preferred way to initialize a dataset**. Unlike test cases, goldens:

- Only require `input`/`scenario` to initialize
- Store expected results like `expected_output`/`expected_outcome`
- Serve as templates before becoming fully-formed test cases

For multi-turn chatbots we use the `ConversationalGolden`s

A `ConversationalGolden` in `deepeval` is used for creating datasets to evaluate multi-turn chatbots. They can be used to generate conversations using the [`ConversationSimulator`](https://deepeval.com/docs/conversation-simulator). Here's the data model of `ConversationalGolden`:

```python
from pydantic import BaseModel

class ConversationalGolden(BaseModel):
    scenario: str
    expected_outcome: Optional[str] = None
    user_description: Optional[str] = None

    # Useful metadata for generating test cases
    additional_metadata: Optional[Dict] = None
    comments: Optional[str] = None
    custom_column_key_values: Optional[Dict[str, str]] = None
```

Here's how you can create a `ConversationalGolden`:

```python
from deepeval.dataset import ConversationalGolden

ConversationalGolden(
    scenario="A user suffering from severe headache and fever consults for medical advice.",
    expected_outcome="Provides proper measures to take and offers to book appointment if it's too severe.",
    additional_metadata={
        "user_intentions": {
            "reporting new symptoms and seeking advice": 1,
            "asking about medication side effects": 2,
            "inquiring about illness prevention": 1,
        },
        "user_profiles": [
            "Peter Parker got bit by a spider and is experiencing headache, fever.",
            "Bruce Wayne suffering from insomnia and is unable to sleep."
        ]
    }
)
```

Here's how we can create a dataset with the goldens we've created:

```python
from deepeval.dataset import EvaluationDataset, ConversationalGolden

dataset = EvaluationDataset(
    goldens=[
        ConversationalGolden(
            scenario="A user suffering from severe headache and fever consults for medical advice.",
            expected_outcome="Provides proper measures to take and offers to book appointment if it's too severe.",
            additional_metadata={
                "user_intentions": {
                    "reporting new symptoms and seeking advice": 1,
                    "asking about medication side effects": 2,
                    "inquiring about illness prevention": 1,
                },
                "user_profiles": [
                    "Peter Parker got bit by a spider and is experiencing headache, fever.",
                    "Bruce Wayne suffering from insomnia and is unable to sleep."
                ]
            }
        ),
        ...
    ]
)
print(dataset._multi_turn) # prints True
```


### Saving Dataset

Now that we have our dataset, we can store this dataset in the Confident AI cloud platform and pull it anytime to generate test cases allowing us to evaluate our chatbot. Here's how we can store our dataset in Confident AI.

```python
dataset.push(alias="Medical Chatbot Dataset")
```

Here's how your dataset would look like on the Confident AI platform:

![Dataset on Confident AI Platform](https://deepeval-docs.s3.us-east-1.amazonaws.com/tutorials:medical-chatbot:dataset-platform.png)


:::note
You must be logged in to your [Confident AI](https://confident-ai.com) account to manage datasets on cloud. Set up Confident AI as shown [here](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai) or just run the following code in your terminal to get started:
```bash
deepeval login
```
:::

You can later pull this dataset to generate test cases and use them to evaluate your chatbot. 

In the next section, we are going to see how to [improve our medical chatbot](/tutorials/medical-chatbot/tutorial-medical-chatbot-improvement) by iterating over multiple hyperparameters and evaluating them using `deepeval`.