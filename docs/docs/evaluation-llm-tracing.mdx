---
id: evaluation-llm-tracing
title: LLM Tracing
sidebar_label: Tracing
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import VideoDisplayer from "@site/src/components/VideoDisplayer";
import { Timeline, TimelineItem } from "@site/src/components/Timeline";

Tracing your LLM application helps you monitor its full execution from start to finish. With `deepeval`'s `@observe` decorator, you can trace and evaluate any [LLM interaction](/docs/evaluation-test-cases#what-is-an-llm-interaction) at any point in your app no matter how complex they may be.

## Quick Summary

In an LLM app, each component can be grouped into a **span** — a flexible, user-defined scope for evaluation or debugging. A full run of your app forms a **trace**, which consists of one or more spans.

<Tabs>

<TabItem value="traces" label="Evals on Traces">

Evals on traces are [end-to-end evaluations](/docs/evaluation-end-to-end-llm-evals), where a single LLM interaction is being evaluated.

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4"
  confidentUrl="/docs/llm-tracing/introduction"
  label="Learn how to setup LLM tracing for Confident AI"
/>

</TabItem>

<TabItem value="spans" label="Evals on Spans">

Spans make up a trace and evals on spans represents [component-level evaluations](/docs/evaluation-component-level-llm-evals), where individual components in your LLM app are being evaluated.

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:spans.mp4"
  confidentUrl="/docs/llm-tracing/introduction"
  label="Learn how to setup LLM tracing for Confident AI"
/>

</TabItem>

</Tabs>

<details>

<summary>Learn how DeepEval's tracing is non-instrusive</summary>

`deepeval`'s tracing is **non-intrusive**, requires **minimal code change** and **doesn't add latency** to your LLM application. It also:

- **Uses concepts you already know**: Tracing a component in your LLM app takes on average 3 lines of code, which uses the same `LLMTestCase`s and [metrics](/docs/metrics-introduction) that you're already familiar with.

- **Does not affect production code**: If you're worried that tracing will affect your LLM calls in production, it won't. This is because the `@observe` decorators that you add for tracing is only invoked if called explicitly during evaluation.

- **Non-opinionated**: `deepeval` does not care what you consider a "component" - in fact a component can be anything, at any scope, as long as you're able to set your `LLMTestCase` within that scope for evaluation.

Tracing only runs when you want it to run, and takes 3 lines of code:

```python showLineNumbers {3,8,15}
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.tracing import observe, update_current_span
from openai import OpenAI

client = OpenAI()

@observe(metrics=[AnswerRelevancyMetric()])
def complete(query: str):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": query}]
    ).choices[0].message.content

    update_current_span(test_case=LLMTestCase(input=query, output=response))
    return response
```

</details>

### Why Tracing?

- **Run targeted metrics on specific components:** Attach `LLMTestCase`s to agents, tools, retrievers, or LLMs and apply metrics like answer relevancy or context precision—without needing to restructure your app.

- **Generate test cases dynamically:** Many components rely on upstream outputs. Tracing lets you define `LLMTestCase`s at runtime as data flows through the system.

- **Debug with precision:** See exactly where and why things fail—whether it’s tool calls, intermediate outputs, or context retrieval steps.

- **Run end-to-end evals with trace data:** Use the `evals_iterator` with `metrics` to perform comprehensive evaluations using your traces.

## Setup Tracing

To set up tracing in your LLM app, you need to understand two key concepts:

* **Trace**: The full execution of your app, made up of one or more spans.
* **Span**: A specific component or unit of work—like an LLM call, tool invocation, or document retrieval.

Spans can be nested to reflect how your components interact, forming a tree-like structure. You attach metrics to spans to evaluate specific parts of your app.


<Timeline>
<TimelineItem title="Decorate Your Components">

You need to first find all the different components of your LLM application that you can be grouped into individual spans. Then add an `@observe` decorator on top of those components.

```python showLineNumbers {2,6}
from openai import OpenAI
from deepeval.tracing import observe

client = OpenAI()

@observe()
def complete(query: str):
    pass
```

Learn more about the `@observe` decorator [here](#observe).

</TimelineItem>

<TimelineItem title="Add Test Cases Inside Components">

When you're doing component-level evals, you need to create test cases at each component level for evaluations to take place. You can do this using the `update_current_span` method from `deepeval`.

```python showLineNumbers {2-3,14-16}
from openai import OpenAI
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase

client = OpenAI()

@observe()
def complete(query: str):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": query}]
    ).choices[0].message.content

    update_current_span(
        test_case=LLMTestCase(input=query, output=response)
    )
    return response
```

To run **component-level evaluations**, you **must**:

- Supply the `metrics` parameter in `@observe`
- Call `update_current_span()` at runtime with an `LLMTestCase`

If you simply decorate your LLM application with `@observe` and don't supply any arguments, nothing will happen at all.

</TimelineItem>
<TimelineItem title="Add Metrics To Components">

You can now add `metrics` and specify the `type`s of span for each decorator to enable component level evals and get better visibility of your LLM application.

```python showLineNumbers {2,8}
from openai import OpenAI
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.tracing import observe, update_current_span

client = OpenAI()

@observe(metrics=[AnswerRelevancyMetric()], type="llm")
def complete(query: str):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": query}]
    ).choices[0].message.content

    update_current_span(
        test_case=LLMTestCase(input=query, output=response)
    )
    return response
```

:::info
The `metrics` parameter is optional because some users might want to use tracing only for the [debugging UI on Confident AI](https://www.confident-ai.com/docs/llm-tracing/introduction), which is also possible when running end-to-end evaluations.
:::

</TimelineItem>
</Timeline>

### Observe

The `@observe` decorator is a non-intrusive python decorator that you can use on top of any component as you wish. It tracks the usage of the component whenever it is evoked to create a span.

```python showLineNumbers
from deepeval.tracing import observe

@observe()
def component_one(query: str) -> str:
    # Your implementation
    return "You output for component one"

@observe()
def component_two(query: str) -> str:
    # Your implementation
    return "You output for component two"
```

There are **THREE** optional parameters when using the `@observe` decorator:

- [Optional] `metrics`: A list of metrics of type `BaseMetric` that will be used to evaluate your span. Defaulted to `None`.
- [Optional] `metric_collection`: The name of the metric collection you stored in the Confident AI platform.
- [Optional] `name`: A string specifying how this custom span is displayed on Confident AI. Defaults to function name.
- [Optional] `type`: A string specifying the type of span. The value can be any one of `llm`, `retriever`, `tool`, and `agent`. Any other value is treated as a custom span type.

<details>
<summary><strong>Click here to learn more about span types</strong></summary>

For simplicity, we always recommend **custom spans** unless needed otherwise, since `metrics` only care about the scope of the span, and supplying a specified `type` is most **useful only when using Confident AI**. To summarize:

- Specifying a span type (like `"llm"`) allows you to supply additional parameters in the `@observe` signature (e.g., the `model` used).
- This information becomes extremely useful for analysis and visualization if you're using `deepeval` together with **Confident AI** (highly recommended).
- Otherwise, for local evaluation purposes, span `type` makes **no difference** — evaluation still works the same way.

To learn more about the different spans `type`s, or to run LLM evaluations with tracing with an UI for visualization and debugging, visiting the [official Confident AI docs on LLM tracing.](https://www.confident-ai.com/docs/llm-tracing/introduction)

</details>

### Update Current Span

To run component level evals, each component has to have a test case of its own. We can use the `update_current_span` method to create a test case and assign it to the current component.

```python showLineNumbers
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase

@observe()
def component_one(query: str) -> str:
    # Your implementation
    res = "You output for component one"
    update_current_span(test_case=LLMTestCase(input=query, actual_output=res))
    return res 

@observe()
def component_two(query: str) -> str:
    # Your implementation
    res = "You output for component two"
    update_current_span(test_case=LLMTestCase(input=query, actual_output=res))
    return res
```

There are **TEN** optional parameters when using the `update_current_span` function:

- [Optional] `test_case`: You can create a test case for this span by giving an `LLMTestCase` here.

- You can also opt to give the values of `LLMTestCase` directly by using the following attributes:
    - [Optional] `input`
    - [Optional] `output`
    - [Optional] `retrieval_context`
    - [Optional] `context`
    - [Optional] `expected_output`
    - [Optional] `tools_called`
    - [Optional] `expected_tools`
    - [Optional] `metadata`
    - [Optional] `name`

### Running Evals

You can now simply use the dataset's `evals_iterator` (or `assert_test()` with `deepeval test run`) to run evaluations:

<Tabs>
<TabItem value="component-level" label="Component Level">

Call your LLM application with the correct parameters and `deepeval` automatically allows you to do component-level evals.

```python {9}
from deepeval.dataset import Golden, EvaluationDataset

# Create golden instead of test case
golden = Golden(input="What's the weather like in SF?")
dataset = EvaluationDataset(goldens=[golden])

# Run evaluation
for golden in dataset.evals_iterator():
    research_agent(golden.input)
```

</TabItem>
<TabItem value="end-to-end" label="End-to-End">

Add `metrics` in your `evals_iterator` method and run end-to-end evals for your application.

```python {9}
from deepeval.dataset import Golden, EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

# Create golden instead of test case
golden = Golden(input="What's the weather like in SF?")
dataset = EvaluationDataset(goldens=[golden])

# Run evaluation
for golden in dataset.evals_iterator(metrics=[AnswerRelevancyMetric()]):
    research_agent(golden.input)
```

</TabItem>
</Tabs>

<details>
<summary><strong>Verbose Logs Outside of <code>evaluate()</code> or <code>assert_test()</code> </strong></summary>

If you run your `@observe` decorated LLM application outside of `evaluate()` or `assert_test()`, you will see logs appearing in your console. These logs are intended for debugging and help users using Confident AI alongside `deepeval` verify that their LLM application is correctly set up for monitoring during development.

If you're not using Confident AI, you can safely ignore these logs — they won't affect anything, introduce any latency, block any process, etc. To disable them entirely, set the following environment variables:

```bash
CONFIDENT_TRACE_VERBOSE="NO"
CONFIDENT_TRACE_FLUSH="NO"
```

If you're using Confident AI, you may still want to set these environment variables in production once you've verified LLM tracing is working correctly for you.

</details>


## Disclaimers

Throughout this documentation, you may notice references to features we didn't cover in detail (such as span `type`s, `name`s, etc.). These additional features are fully addressed in [Confident AI's documentation on LLM tracing](https://www.confident-ai.com/docs/llm-tracing/introduction), which we recommend reading if you're interested in setting up an advanced LLM evaluation & observability suite.

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4"
  confidentUrl="/docs/llm-tracing/introduction"
  label="Learn how to setup LLM tracing for Confident AI"
/>

:::note
LLM tracing in `deepeval` offers fully functionality for component-level evaluation, but when used alongside Confident AI as we recommend, it enables more advanced features like end-to-end debugging.
:::
