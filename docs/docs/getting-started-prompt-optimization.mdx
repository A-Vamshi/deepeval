---
id: getting-started-prompt-optimization
title: Prompt Optimization
sidebar_label: Prompt Optimization
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplayer from "@site/src/components/VideoDisplayer";

## Prerequisites

- Install `deepeval`
- A Confident AI API key (recommended). Sign up for one [here.](https://app.confident-ai.com)

:::info
Confident AI allows you to view and share your testing reports. Set your API key in the CLI:

```bash
CONFIDENT_API_KEY="confident_us..."
```

:::

## Setup Prompt Versioning

Prompt optimization requires testing multiple variations of a prompt and selecting the best-performing one. This means that having multiple versions of a prompt is a prerequisite.

<Timeline>
<TimelineItem title="Create a prompt version">

Create a new prompt version by providing an alias and pushing the prompt to Confident AI.

<Tabs>
<TabItem value="platform" label="Messages Prompt">

```python title="main.py" showLineNumbers={true}
from deepeval.prompt import Prompt

prompt = Prompt(alias="New Prompt")
prompt.push(model="gpt-4o", messages=[{"role": "system", "content": "You are an AI Chow Chow."}])
```

</TabItem>
<TabItem value="text" label="Text Prompt">

```python title="main.py" showLineNumbers={true}
from deepeval.prompt import Prompt

prompt = Prompt(alias="New Prompt")
prompt.push(model="gpt-4o", text="You are an AI Chow Chow.")
```

</TabItem>
</Tabs>

You should see a confirmation log like the one below in your CLI if you're logged in to Confident AI:

<pre>
  <code>
    <span style={{ color: "#FFFFFF", whiteSpace: "nowrap" }}>
      ✅ Prompt successfully pushed to Confident AI! View at{" "}
    </span>
    <span
      style={{
        color: "#5f5fff",
        textDecoration: "underline",
        whiteSpace: "nowrap",
      }}
    >
      https://app.confident.ai/[...]
    </span>
  </code>
</pre>

<details>
<summary>Click to see all available prompt attributes</summary>

You can also associate the following attributes with a prompt version:

```python title="main.py" group="prompt-example"
from deepeval.prompt import Prompt, OutputFormat, ReasoningEffort, Verbosity

prompt = Prompt(alias="New Prompt")
prompt.push(
    ...,
    model="gpt-4o",
    max_tokens=100,
    temperature=0.7,
    top_p=0.9,
    frequency_penalty=0.2,
    presence_penalty=0.2,
    reasoning_effort=ReasoningEffort.MEDIUM,
    verbose=Verbosity.MEDIUM,
    output_type=OutputType.TEXT,
    output_schema=OutputFormat,
)
```

</details>

</TimelineItem>
<TimelineItem title="Commit a new version">

To commit a new version of a prompt, simply push the prompt again under the same alias. This will automatically create a version `00.00.02` of the prompt on Confident AI.

<Tabs>
<TabItem value="platform" label="Messages Prompt">

```python title="main.py" showLineNumbers={true}
from deepeval.prompt import Prompt

prompt = Prompt(alias="New Prompt")
prompt.push(
    model="gpt-4o",
    messages=[{"role": "system", "content": "You are an AI Chow Chow. You can only speak in dog language."}]
)
```

</TabItem>
<TabItem value="text" label="Text Prompt">

```python title="main.py" showLineNumbers={true}
from deepeval.prompt import Prompt

prompt = Prompt(alias="New Prompt")
prompt.push(
    model="gpt-4o",
    text="You are an AI Chow Chow. You can only speak in dog language."
)
```

</TabItem>
</Tabs>

</TimelineItem>

<TimelineItem title="Maintain on Confident AI (recommended)">

Click on the link in the confirmation log to view your prompt on [Confident AI](https://app.confident-ai.com)'s prompt studio. Here, you can not only view but also create, edit, and version your prompts without writing code.

<VideoDisplayer src="https://confident-docs.s3.us-east-1.amazonaws.com/prompts:create-messages-4k.mp4" />

</TimelineItem>
</Timeline>

## Run Your First Evals

<Timeline>

<TimelineItem title="Pull your prompt">

Pull your desired prompt version from Confident AI and call `interpolate` to retrieve the prompt message. For this example, we'll assume `New Prompt` is a message prompt.

```python title="main.py" showLineNumbers={true}
from deepeval.prompt import Prompt

prompt = Prompt(alias="New Prompt")
prompt.pull(version="00.00.01")
prompt_v1 = prompt.interpolate()
```

<details>
<summary>Click to see <code>prompt_v1</code></summary>

If your prompt contains dynamic variables that is only accessible at runtime, you can pass in these variables to the `interpolate` method. Otherwise, you interpolate the prompt without any arguments.

```json
[
  {
    "role": "system",
    "content": "You are an AI Chow Chow. You can only speak in dog language."
  }
]
```

</details>

</TimelineItem>

<TimelineItem title="Create a test case">

Prepare a test case by using the prompt version you retrieved to generate your LLM application's output.

<Tabs>
<TabItem value="python" label="Python">

```python title="main.py" showLineNumbers={true} {5}
from deepeval.test_case import LLMTestCase
...

input = "Who's a good boy?"
actual_output = my_llm_function(input, prompt_v1)
test_case = LLMTestCase(input=input, actual_output=actual_output)
```

</TabItem>
<TabItem value="openai" label="OpenAI">

```python title="main.py" showLineNumbers={true} {10}
from deepeval.test_case import LLMTestCase
from openai import OpenAI
...

client = OpenAI()
input = "Who's a good boy?"

response = client.chat.completions.create(
    model="gpt-4o",
    messages=prompt_v1 + [{"role": "user", "content": input}]
)
test_case = LLMTestCase(input=input, actual_output=response.choices[0].message.content)
```

</TabItem>
<TabItem value="anthropic" label="Anthropic">

```python title="main.py" showLineNumbers={true} {10}
from deepeval.test_case import LLMTestCase
import anthropic
...

client = anthropic.Anthropic()
input = "Who's a good boy?"

message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=prompt_v1 + [{"role": "user", "content": input}]
)
test_case = LLMTestCase(input=input, actual_output=message.content[0].text)
```

</TabItem>
<TabItem value="langgraph" label="LangGraph">

```python title="main.py" showLineNumbers={true} {12}
from deepeval.test_case import LLMTestCase
from langgraph.prebuilt import create_react_agent
...

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt=prompt_v1
)

input = "Who's a good boy?"
actual_output = agent.invoke({"messages": [{"role": "user", "content": input}]})
test_case = LLMTestCase(input=input, actual_output=actual_output)
```

</TabItem>
<TabItem value="langchain" label="LangChain">

```python title="main.py" showLineNumbers={true} {10}
from deepeval.test_case import LLMTestCase
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
...

input = "Who's a good boy?"

llm = ChatOpenAI(model="gpt-4o")
prompt_template = ChatPromptTemplate.from_messages([
    (prompt_v1[0]["role"], prompt_v1[0]["content"]),
    ("user", "{input}")
])

chain = prompt_template | llm
actual_output = chain.invoke({"input": input}).content
test_case = LLMTestCase(input=input, actual_output=actual_output)
```

</TabItem>
<TabItem value="pydantic" label="Pydantic AI">

```python title="main.py" showLineNumbers={true} {7}
from deepeval.test_case import LLMTestCase
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
...

model = OpenAIModel('gpt-4o')
agent = Agent(model, system_prompt=prompt_v1[0]["content"])

input = "Who's a good boy?"
result = agent.run_sync(input)
test_case = LLMTestCase(input=input, actual_output=result.data)
```

</TabItem>
</Tabs>

</TimelineItem>

<TimelineItem title="Run an eval">

Run an evaluation on the test case and log the prompt object in the `hyperparameters` field.

```python title="main.py" {6}
from deepeval.metrics import AnswerRelevancy
from deepeval import evaluate
...

evaluate(
    hyperparameters={"prompt": prompt},
    test_cases=[test_case],
    metrics=[AnswerRelevancy()]
)
```

If successful, you should see a confirmation log like the one below in your CLI:

<pre>
  <code>
    <span style={{ color: "#FFFFFF", whiteSpace: "nowrap" }}>
      ✅ Prompt successfully logged to test run! View at{" "}
    </span>
    <span
      style={{
        color: "#5f5fff",
        textDecoration: "underline",
        whiteSpace: "nowrap",
      }}
    >
      https://app.confident.ai/[...]
    </span>
  </code>
</pre>

:::caution
Make sure to pass the prompt object in the `hyperparameters` field and not the
interpolated prompt.
:::

</TimelineItem>

<TimelineItem title="Run your second eval">

Now that we've run our first eval on the first version of our prompt, let's run our second eval. This time, pass in the version `00.00.02` of your prompt instead.

```python title="main.py" showLineNumbers={true} {6, 10}
from deepeval.prompt import Prompt
from deepeval.metrics import AnswerRelevancy
from deepeval import evaluate

prompt = Prompt(alias="New Prompt")
prompt.pull(version="00.00.02")
prompt_v2 = prompt.interpolate()

input = "Who's a good boy?"
actual_output = my_llm_function(input, prompt_v2)
test_case = LLMTestCase(input=input, actual_output=actual_output)

evaluate(
    hyperparameters={"prompt": prompt},
    test_cases=[test_case],
    metrics=[AnswerRelevancy()]
)
```

✅ Done! Now that we've evaluated both of our prompt versions, we now have the resources to compare the two versions and select the best one.

</TimelineItem>
</Timeline>

## Optimize Prompts

<Timeline>
<TimelineItem title="Selecting the test runs">

</TimelineItem>
<TimelineItem title="Comparing test runs">

</TimelineItem>

<TimelineItem title="Drawing Insights">

</TimelineItem>
</Timeline>
