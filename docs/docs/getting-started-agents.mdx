---
id: getting-started-agents
title: LLM Evals for AI Agents
sidebar_label: AI Agents
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

Learn how to evaluate AI Agents using DeepEval, including multi-agent systems and tool-using agents.

## Overview

AI agent evaluation is different from other types of evals because agentic workflows are complex and **consist of multiple interacting components**, such as tools, chained LLM calls, and RAG modules. Therefore, it’s important to evaluate your AI agents both end-to-end and at the component level to understand how each part performs.

**In this 5 min quickstart, you'll learn how to:**

1. Set up LLM tracing for your agent
2. Evaluate your agent end-to-end
3. Evaluate individual components in your agent

## Prerequisites

- Install `deepeval`
- A [Confident AI account](https://app.confident-ai.com) to get an API key

:::info
We recommend logging in to Confident AI to view your evaluation traces:

```bash
deepeval login
```

:::

## Setup LLM Tracing

<Timeline>
<TimelineItem title="Choose your techstack">

<Tabs groupId="techstack">
<TabItem value="python" label="Python">

Attach the <code>@observe</code> decorator to functions/methods that make up your agent. These will represent individual components in your agent.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{3,7}"
from deepeval.tracing import observe

@observe()
def your_ai_agent_tool():
    return 'tool call result'

@observe()
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    return 'Tool Call Result: ' + tool_call_result
```

</TabItem>
<TabItem value="langgraph" label="LangGraph" showLineNumbers={true}>

Pass in `deepeval`'s `CallbackHandler` for LangGraph to your agent's invoke method.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,16}"
from deepeval.integrations.langchain import CallbackHandler
from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="openai:gpt-4.1",
    tools=[get_weather],
    prompt="You are a helpful assistant"
)

result = agent.invoke(
    input={"messages":[{"role":"user","content":"what is the weather in sf"}]},
    config={"callbacks":[CallbackHandler()]}
)
```

</TabItem>
<TabItem value="langchain" label="LangChain">

Pass in `deepeval`'s `CallbackHandler` for LangChain to your agent's invoke method.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,12}"
from deepeval.integrations.langchain import CallbackHandler
from langchain.chat_models import init_chat_model

def multiply(a: int, b: int) -> int:
    return a * b

llm = init_chat_model("gpt-4.1", model_provider="openai")
llm_with_tools = llm.bind_tools([multiply])

llm_with_tools.invoke(
    "What is 3 * 12?",
    config = {"callbacks": [CallbackHandler()]}
)
```

</TabItem>
<TabItem value="crewai" label="CrewAI">

Instrument `deepeval` as an span handler for CrewAI and import your `Agent` from `deepeval` instead.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,6}"
from deepeval.integrations.crewai import instrumentator, Agent
from crewai import Task, Crew

instrumentator(api_key="<your-confident-api-key>")

coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
)

task = Task(
    description="Explain the latest trends in AI.",
    agent=coder,
    expected_output="A clear and concise explanation.",
)

crew = Crew(agents=[coder], tasks=[task])
result = crew.kickoff()
```

</TabItem>
</Tabs>

</TimelineItem>
<TimelineItem title="Configure environment variables">

This will prevent traces from being lost in case of an early program termination.

```bash
export CONFIDENT_TRACE_FLUSH=YES
```

</TimelineItem>
<TimelineItem title="Invoke your agent">

Run your agent as you would normally do:

```bash
python main.py
```

✅ Done. you should see a trace log like the one below in your CLI if you're logged in to Confident AI:

<pre>
  <code>
    <span
      style={{ color: "#7f7f7f", fontWeight: "bold", whiteSpace: "nowrap" }}
    >
      [Confident AI Trace Log]{"  "}
    </span>
    <span style={{ color: "#00ff00", whiteSpace: "nowrap" }}>
      Successfully posted trace (...):{" "}
    </span>
    <span
      style={{
        color: "#5f5fff",
        textDecoration: "underline",
        whiteSpace: "nowrap",
      }}
    >
      https://app.confident.ai/[...]
    </span>
  </code>
</pre>

</TimelineItem>
</Timeline>

## Evaluate Your Agent End-to-End

An [end-to-end evaluation](/docs/evaluation-end-to-end-llm-evals) means your agent will be treated as a black-box, where all that matters is the degree of task completion for a particular invocation.

<Timeline>
<TimelineItem title="Configure evaluation model">

To configure OpenAI as the your evaluation model for all metrics, set your `OPENAI_API_KEY` in the CLI:

```bash
export OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>
```

:::info
You can also use these models for evaluation: [Ollama](https://deepeval.com/integrations/models/ollama), [Azure OpenAI](https://deepeval.com/integrations/models/azure-openai), [Anthropic](https://deepeval.com/integrations/models/anthropic), [Gemini](https://deepeval.com/integrations/models/gemini), etc. To use **ANY** custom LLM of your choice, [check out this part of the docs](/guides/guides-using-custom-llms).
:::

</TimelineItem>
<TimelineItem title="Define metrics">

_Task Completion_ is the most powerful metric on `deepeval` for evaluating AI agents end-to-end.

```python
from deepeval.metrics import TaskCompletionMetric

task_completion_metric = TaskCompletionMetric()
```

The task completion metric works by analyzing traces to determine the task at hand and the degree of completion of said task.

<details>
  <summary>What other metrics are available?</summary>

Other metrics on `deepeval` can also be used to evaluate agents but _ONLY_ if you run [component-level evaluations](/docs/getting-started-agents#component-level-evaluations), since they require you to set up an LLM test case. These metrics include:

- [Tool Correctness](/docs/metrics-tool-correctness)
- [G-Eval](/docs/metrics-llm-evals)
- [Answer Relevancy](/docs/metrics-answer-relevancy)
- [Faithfulness](/docs/metrics-faithfulness)

For more information on available metrics, see the [Metrics Introduction](/docs/metrics-introduction) section.

</details>

</TimelineItem>
<TimelineItem title="Run an evaluation">

Use the `dataset` iterator to invoke your agent with a list of goldens. This will benchmark your agent for this point-in-time and create a test run.

<Tabs groupId="techstack">
<TabItem value="python" label="Python">

Supply the task completion metric to the metrics argument inside the `@observe` decorator.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,9,14}"
from deepeval.tracing import observe
from deepeval.dataset import EvaluationDataset, Golden
...

@observe()
def your_ai_agent_tool():
    return 'tool call result'

@observe(metrics=[task_completion_metric])
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    return 'Tool Call Result: ' + tool_call_result

dataset = EvaluationDataset(goldens=[Golden(input="This is a test query")])
for golden in dataset.as_iterator():
    your_ai_agent(golden.input)
```

</TabItem>
<TabItem value="langgraph" label="LangGraph" showLineNumbers={true}>

Supply the task completion metric to the metrics argument inside the `CallbackHandler`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{3,16,20}"
from deepeval.integrations.langchain import CallbackHandler
from langgraph.prebuilt import create_react_agent
from deepeval.dataset import EvaluationDataset, Golden
...

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="openai:gpt-4.1",
    tools=[get_weather],
    prompt="You are a helpful assistant",
)

dataset = EvaluationDataset(goldens=[Golden(input="This is a test query")])
for golden in dataset.as_iterator():
    agent.invoke(
        input={"messages": [{"role": "user", "content": golden.input}]},
        config={"callbacks": [CallbackHandler(metrics=[task_completion_metric])]},
    )
```

</TabItem>
<TabItem value="langchain" label="LangChain">

Supply the task completion metric to the metrics argument inside the `CallbackHandler`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{3,12,16}"
from deepeval.integrations.langchain import CallbackHandler
from langchain.chat_models import init_chat_model
from deepeval.dataset import EvaluationDataset, Golden
...

def multiply(a: int, b: int) -> int:
    return a * b

llm = init_chat_model("gpt-4.1", model_provider="openai")
llm_with_tools = llm.bind_tools([multiply])

dataset = EvaluationDataset(goldens=[Golden(input="This is a test query")])
for golden in dataset.as_iterator():
    llm_with_tools.invoke(
        "What is 3 * 12?",
        config={"callbacks": [CallbackHandler(metrics=[task_completion_metric])]},
    )
```

</TabItem>
<TabItem value="crewai" label="CrewAI">

Supply the task completion metric to the metrics argument inside the `Agent` from `deepeval`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,12,15}"
from deepeval.integrations.crewai import instrumentator, Agent
from deepeval.dataset import EvaluationDataset, Golden
from crewai import Task, Crew
...

instrumentator(api_key="<your-confident-api-key>")

coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
    metrics=[task_completion_metric]
)

dataset = EvaluationDataset(goldens=[Golden(input="Explain the latest trends in AI.")])
for golden in dataset.as_iterator():
    task = Task(description="Explain the latest trends in AI.", agent=coder, expected_output="A clear and concise explanation.")
    crew = Crew(agents=[coder], tasks=[task])
    result = crew.kickoff()
```

</TabItem>
</Tabs>
</TimelineItem>
<TimelineItem title="View on Confident AI">

If you're logged in, you can debug your traces and evals on [Confident AI](https://app.confident-ai.com), the DeepEval platform.

:::tip
If you're on Confident AI, we recommend going directly to the [tracing docs.](https://documentation.confident-ai.com/docs/llm-tracing/introduction)
:::

<Tabs groupId="techstack">
<TabItem value="python" label="Python">

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started:ai-agent-evals:end-to-end.mp4" />

</TabItem>
<TabItem value="langgraph" label="LangGraph">

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started:ai-agent-evals:langgraph.mp4" />

</TabItem>
<TabItem value="langchain" label="LangChain">

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started:ai-agent-evals:langchain.mp4" />

</TabItem>
<TabItem value="crewai" label="CrewAI">

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started:ai-agent-evals:crew-ai.mp4" />

</TabItem>
</Tabs>

</TimelineItem>
</Timeline>

## Evaluate Agentic Components

[Component-level evaluations](/docs/getting-started-agents#component-level-evaluations) treats your agent as a white box, allowing you to isolate and evaluate the performance of individual components in your agent.

:::caution
Component-level evaluation integrations are **not available** on DeepEval at this time. If you're building with an LLM framework, manually trace your agent with `@observe` decorators to evaluate agentic components.
:::

<Timeline>
<TimelineItem title="Define metrics">

Any [single-turn metric](/docs/metrics-introduction) can be used to evaluate agentic components.

```python title=main.py showLineNumbers={true} wrapLines={true}
from deepeval.metrics import TaskCompletionMetric, ToolCorrectnessMetric

tool_correctness_metric = ToolCorrectnessMetric()
task_completion_metric = TaskCompletionMetric()
```

:::tip
What metrics you ultimately choose depend on the types of components you're evaluating. For example, [Tool Correctness](/docs/metrics-tool-correctness) is suitable for evaluating tool-calling components, while [RAG metrics](/docs/metrics-answer-relevancy) are useful for retrievers.
:::

</TimelineItem>
<TimelineItem title="Set up metrics">

Supply the metrics to the `@observe` decorator of each function, then define a test case in `update_span`. The test case should include [parameters](/docs/metrics-introduction#test-case-parameters) required by the metrics you supply.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{5,9,10,11,12,17,23,24}"
from deepeval.tracing import observe, update_span
from deepeval.test_case import LLMTestCase, ToolCall
...

@observe(metrics=[tool_correctness_metric])
def your_ai_agent_tool(input):
    update_span(
        test_case=LLMTestCase(
            input=input,
            actual_output='tool call result',
            tools_called=[ToolCall(name='Tool A')],
            expected_tools=[ToolCall(name='Tool A')]
        )
    )
    return 'tool call result'

@observe(metrics=[task_completion_metric])
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    output = 'Tool Call Result: ' + tool_call_result
    update_span(
        test_case=LLMTestCase(
            input=input,
            actual_output=output
        )
    )
    return output
```

</TimelineItem>
<TimelineItem title="Run an evaluation">

Finally, use the `dataset` iterator to invoke your agent on a list of goldens.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.dataset import EvaluationDataset, Golden
...

dataset = EvaluationDataset(goldens=[Golden(input='This is a test query')])
for golden in dataset.as_iterator():
    tool_call_result = your_ai_agent(golden.input)
```

</TimelineItem>

<TimelineItem title="View on Confident AI">

If you're logged in, `deepeval` will automatically generate a test report on [Confident AI](https://app.confident.ai), DeepEval's platform.

:::tip
We recommend logging in to Confident AI to view and debug your evaluation traces in detail:

```bash
deepeval login
```

:::

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aai-agent-evals%3Aend-to-end.mp4" />

</TimelineItem>
</Timeline>
