---
id: getting-started-agents
title: Evaluating AI Agents
sidebar_label: AI Agent Evals
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

DeepEval makes it easy to evaluate complex **AI Agents**, such as multi-agent systems and tool-using agents.

<details>
  <summary>Click to learn more about AI Agent Evaluation</summary>

AI Agent Evaluation is different from other types of evaluations because agentic workflows are complex and **consist of multiple interacting components**, such as tools, chained LLM calls, and RAG modules.

Therefore, it’s important to evaluate your AI agents both end-to-end and at the component level to understand how each part performs. This involves [tracing each component in your agent](/docs/getting-started-agents#setup-tracing).

[Task Completion](/docs/metrics-task-completion) is the most powerful and recommended metric for AI agent evaluations. It analyzes your Agent’s full trace and does not require setting up an LLM test case. Other metrics on DeepEval can also be used to evaluate agents, but they require you to set up an LLM test case. Some of these include:

- [Tool Correctness](/docs/metrics-tool-correctness)
- [G-Eval](/docs/metrics-llm-evals)
- [Answer Relevancy](/docs/metrics-answer-relevancy)
- [Faithfulness](/docs/metrics-faithfulness)

</details>

In this guide, you'll learn how to set up your agent for evaluations in **under 5 minutes**:

1. Set up tracing for your agent.
2. Evaluate your agent end-to-end.
3. Evaluate components in your agent.

:::info
You'll need to **log in to Confident AI** to view your evaluation traces:

```bash
deepeval login
```

:::

## Setup Tracing

<Timeline>
<TimelineItem title="Setup tracing">

<Tabs>
<TabItem value="python" label="Python">

Attach the <code>@observe</code> decorator to all the functions that are part of your agent.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4,8}"
from deepeval.tracing import observe
...

@observe()
def your_ai_agent_tool():
    return 'tool call result'

@observe()
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    return 'Tool Call Result: ' + tool_call_result
```

</TabItem>
<TabItem value="langgraph" label="LangGraph" showLineNumbers={true}>

Pass in DeepEval's `CallbackHandler` for LangGraph to your agent's invoke method.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,16}"
from deepeval.integrations.langchain import CallbackHandler
from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="openai:gpt-4.1",
    tools=[get_weather],
    prompt="You are a helpful assistant"
)

result = agent.invoke(
    input={"messages":[{"role":"user","content":"what is the weather in sf"}]},
    config={"callbacks":[CallbackHandler()]}
)
```

</TabItem>
<TabItem value="langchain" label="LangChain">

Pass in DeepEval's `CallbackHandler` for LangChain to your agent's invoke method.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,12}"
from deepeval.integrations.langchain import CallbackHandler
from langchain.chat_models import init_chat_model

def multiply(a: int, b: int) -> int:
    return a * b

llm = init_chat_model("gpt-4.1", model_provider="openai")
llm_with_tools = llm.bind_tools([multiply])

llm_with_tools.invoke(
    "What is 3 * 12?",
    config = {"callbacks": [CallbackHandler()]}
)
```

</TabItem>
<TabItem value="llama-index" label="LlamaIndex">

Instrument DeepEval as an span handler for LlamaIndex and import your `FunctionAgent`, `ReActAgent`, and `CodeActAgent` from DeepEval instead.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,2,6}"
from deepeval.integrations.llama_index import instrument_llama_index, FunctionAgent
import llama_index.core.instrumentation as instrument
from llama_index.llms.openai import OpenAI
import asyncio

instrument_llama_index(instrument.get_dispatcher())

def multiply(a: float, b: float) -> float:
    return a * b

agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)

async def main():
    response = await agent.run("What is 1234 * 4567?")
    print(response)

asyncio.run(main())
```

</TabItem>
<TabItem value="crewai" label="CrewAI">

Instrument DeepEval as an span handler for CrewAI and import your `Agent` from DeepEval instead.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,6}"
from deepeval.integrations.crewai import instrumentator, Agent
from crewai import Task, Crew

instrumentator(api_key="<your-confident-api-key>")

coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
)

task = Task(
    description="Explain the latest trends in AI.",
    agent=coder,
    expected_output="A clear and concise explanation.",
)

crew = Crew(
    agents=[coder],
    tasks=[task],
)
result = crew.kickoff()
```

</TabItem>
<TabItem value="pydantic" label="Pydantic">

_To be documented..._

</TabItem>
<TabItem value="OpenAI Agents" label="OpenAI Agents">

_To be documented..._

</TabItem>
</Tabs>

</TimelineItem>
<TimelineItem title="Configure environment variables">

If you are running evaluations in a local environment, you will need to set the following environment variable. Skip this step if you are running [online evaluations in production](https://documentation.confident-ai.com/docs/llm-tracing/evaluations#online-evaluations)

```bash
export CONFIDENT_TRACE_FLUSH=YES
```

</TimelineItem>
<TimelineItem title="Run a test query">

Run your agent evaluations as you would normally do:

```bash
python main.py
```

If successful, you should see a trace log like the one below in your CLI:

<pre>
  <code>
    <span
      style={{ color: "#7f7f7f", fontWeight: "bold", whiteSpace: "nowrap" }}
    >
      [Confident AI Trace Log]{"  "}
    </span>
    <span style={{ color: "#00ff00", whiteSpace: "nowrap" }}>
      Successfully posted trace (...):{" "}
    </span>
    <span
      style={{
        color: "#5f5fff",
        textDecoration: "underline",
        whiteSpace: "nowrap",
      }}
    >
      https://app.confident.ai/[...]
    </span>
  </code>
</pre>

</TimelineItem>
</Timeline>

## End-to-End Evaluations

<Timeline>
<TimelineItem title="Configure metrics">

_Task Completion_ is the most powerful metric on DeepEval for evaluating AI agents end-to-end.

<details>
  <summary>What other metrics are available?</summary>

Other metrics on DeepEval can also be used to evaluate agents but _ONLY_ if you run [component-level evaluations](/docs/getting-started-agents#component-level-evaluations), since they require you to set up an LLM test case. These metrics include:

- [Tool Correctness](/docs/metrics-tool-correctness)
- [G-Eval](/docs/metrics-llm-evals)
- [Answer Relevancy](/docs/metrics-answer-relevancy)
- [Faithfulness](/docs/metrics-faithfulness)

For more information on available metrics, see the [Metrics Introduction](/docs/metrics-introduction) section.

</details>

<Tabs>
<TabItem value="python" label="Python">

Supply the `TaskCompletionMetric` to the metrics argument inside the observe decorator.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,9}"
from deepeval.tracing import observe
from deepeval.metrics import TaskCompletionMetric
...

@observe()
def your_ai_agent_tool():
    return 'tool call result'

@observe(metrics=[TaskCompletionMetric()])
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    return 'Tool Call Result: ' + tool_call_result
```

</TabItem>
<TabItem value="langgraph" label="LangGraph" showLineNumbers={true}>

Supply the `TaskCompletionMetric` to the metrics argument inside the `CallbackHandler`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,17}"
from deepeval.integrations.langchain import CallbackHandler
from deepeval.metrics import TaskCompletionMetric
from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="openai:gpt-4.1",
    tools=[get_weather],
    prompt="You are a helpful assistant"
)

result = agent.invoke(
    input={"messages":[{"role":"user","content":"what is the weather in sf"}]},
    config={"callbacks":[CallbackHandler(metrics=[TaskCompletionMetric()])]}
)
```

</TabItem>
<TabItem value="langchain" label="LangChain">

Supply the `TaskCompletionMetric` to the metrics argument inside the `CallbackHandler`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,15}"
from deepeval.integrations.langchain import CallbackHandler
from deepeval.metrics import TaskCompletionMetric
from langchain.chat_models import init_chat_model
import time
import os

def multiply(a: int, b: int) -> int:
    return a * b

llm = init_chat_model("gpt-4.1", model_provider="openai")
llm_with_tools = llm.bind_tools([multiply])

llm_with_tools.invoke(
    "What is 3 * 12?",
    config = {"callbacks": [CallbackHandler(metrics=[TaskCompletionMetric()])]}
)
```

</TabItem>
<TabItem value="llama-index" label="LlamaIndex">

Supply the `TaskCompletionMetric` to the metrics argument inside the `FunctionAgent`, `ReActAgent`, and `CodeActAgent` from DeepEval.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{3,16}"
from deepeval.integrations.llama_index import instrument_llama_index, FunctionAgent
import llama_index.core.instrumentation as instrument
from deepeval.metrics import TaskCompletionMetric
from llama_index.llms.openai import OpenAI
import asyncio

instrument_llama_index(instrument.get_dispatcher())

def multiply(a: float, b: float) -> float:
    return a * b

agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
    metrics=[TaskCompletionMetric()]
)

async def main():
    response = await agent.run("What is 1234 * 4567?")
    print(response)

asyncio.run(main())
```

</TabItem>
<TabItem value="crewai" label="CrewAI">

Supply the `TaskCompletionMetric` to the metrics argument inside the `Agent` from DeepEval.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{2,11}"
from deepeval.integrations.crewai import instrumentator, Agent
from deepeval.metrics import TaskCompletionMetric
from crewai import Task, Crew

instrumentator(api_key="q8/AU3bxv2MX0mBnW9I8ynOVNx/iV3mMH3oqkl2Isu4=")

coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
    metrics=[TaskCompletionMetric()]
)

task = Task(
    description="Explain the latest trends in AI.",
    agent=coder,
    expected_output="A clear and concise explanation.",
)

crew = Crew(
    agents=[coder],
    tasks=[task],
)
result = crew.kickoff()
```

</TabItem>
<TabItem value="pydantic" label="Pydantic">

_To be documented..._

</TabItem>
<TabItem value="OpenAI Agents" label="OpenAI Agents">

_To be documented..._

</TabItem>
</Tabs>

</TimelineItem>
<TimelineItem title="Run an evaluation">
<p>
    To run an evaluation, run a query for each golden using the <code>dataset</code> iterator.
</p>

<Tabs>
<TabItem value="python" label="Python">

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.evaluate import dataset
...

for golden in dataset(goldens=[Golden(input='This is a test query')]):
    tool_call_result = your_ai_agent(golden.input)
```

DeepEval will automatically generate a test report on Confident AI, where you can debug and view evaluation results.

<VideoDisplay src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4" />

</TabItem>
<TabItem value="langgraph" label="LangGraph" showLineNumbers={true}>

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.evaluate import dataset
...

for golden in dataset(goldens=[Golden(input='This is a test query')]):
    result = agent.invoke(
        input={"messages":[{"role":"user","content":golden.input}]},
        config={"callbacks":[CallbackHandler(metrics=[TaskCompletionMetric()])]}
    )
```

</TabItem>
<TabItem value="langchain" label="LangChain">

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.evaluate import dataset
...

for golden in dataset(goldens=[Golden(input='This is a test query')]):
    llm_with_tools.invoke(
        "What is 3 * 12?",
        config = {"callbacks": [CallbackHandler(metrics=[TaskCompletionMetric()])]}
    )
```

</TabItem>
<TabItem value="llama-index" label="LlamaIndex">

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.evaluate import dataset
...

for golden in dataset(goldens=[Golden(input='This is a test query')])
    asyncio.run(agent.run(golden.input))
```

</TabItem>
<TabItem value="crewai" label="CrewAI">
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
      metastring="{1,4}"
    >
      {"from deepeval.evaluate import dataset\n" +
        "...\n\n" +
        "for golden in dataset(goldens=[Golden(input='This is a test query')])\n" +
        "    task = Task(\n" +
        "        description=golden.input,\n" +
        "        agent=coder\n" +
        "    )\n" +
        "    crew = Crew(agents=[coder], tasks=[task])\n" +
        "    crew.kickoff()"}
    </CodeBlock>
</TabItem>
<TabItem value="pydantic" label="Pydantic">
 
 To be documented...
 
</TabItem>
<TabItem value="OpenAI Agents" label="OpenAI Agents">
 
 To be documented...
 
</TabItem>
</Tabs>

</TimelineItem>
</Timeline>

## Component-Level Evaluations

Component-level evaluations allow for fine-grained evaluation of specific components in your agent.

:::note
Component-level evaluations are **not available** for integrations at this time.
:::

<Timeline>
<TimelineItem title="Set up test cases">

Set up test cases for each function in your agent. You can learn more about available [test case parameters here.](/docs/evaluation-test-cases)

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{1,7,19}"
from deepeval.tracing import observe, update_span
from deepeval.test_case import LLMTestCase, ToolCall
...

@observe()
def your_ai_agent_tool():
    update_span(
        test_case=LLMTestCase(
            tools_called=[ToolCall(name='Tool A')],
            expected_tools=[ToolCall(name='Tool A')]
        )
    )
    return 'tool call result'

@observe()
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    output = 'Tool Call Result: ' + tool_call_result
    update_span(
        test_case=LLMTestCase(
            input=input,
            actual_output=output
        )
    )
    return output
```

</TimelineItem>
<TimelineItem title="Configure metrics">

Supply the metrics to the `@observe` decorator of each function. Each metric will require different test case parameters.

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{3,6,16}"
from deepeval.tracing import observe, update_span
from deepeval.test_case import LLMTestCase, ToolCall
from deepeval.metrics import TaskCompletionMetric, ToolCorrectness
...

@observe(metrics=[ToolCorrectness()])
def your_ai_agent_tool():
    update_span(
        test_case=LLMTestCase(
            tools_called=[ToolCall(name='Tool A')],
            expected_tools=[ToolCall(name='Tool A')]
        )
    )
    return 'tool call result'

@observe(metrics=[TaskCompletionMetric()])
def your_ai_agent(input):
    tool_call_result = your_ai_agent_tool()
    output = 'Tool Call Result: ' + tool_call_result
    update_span(
        test_case=LLMTestCase(
            input=input,
            actual_output=output
        )
    )
    return output
```

</TimelineItem>
<TimelineItem title="Run Evaluations">

Finally, run a query for each golden using the `dataset` iterator.

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.evaluate import dataset
...

for golden in dataset(goldens=[Golden(input='This is a test query')]):
    tool_call_result = your_ai_agent(golden.input)
```

DeepEval will automatically generate a component-level test report on Confident AI.

<VideoDisplay src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4" />

</TimelineItem>
</Timeline>
