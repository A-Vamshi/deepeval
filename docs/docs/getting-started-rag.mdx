---
id: getting-started-rag
title: LLM Evals for RAG
sidebar_label: RAG
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

DeepEval makes it easy to evaluate any **RAG** pipelines and systems, including QA agents and customer support chatbots.

## Overview

A good [RAG evaluation pipeline](/guides/guides-rag-evaluation) can help you determine the best prompts, models, and retriever settings for your use-case. DeepEval allows you to evaluate your RAG pipelines end-to-end, as components of AI agents, or even inside chatbot systems. RAG workflows are captured by `LLMTestCase`s, which represent a single user-LLM interaction.

![ok](https://deepeval-docs.s3.amazonaws.com/docs:llm-test-case.png)

**In this 5 min quickstart, you'll learn how to:**

1. Evaluate your RAG pipeline end-to-end.
2. Evaluate RAG components in your LLM application.
3. Evaluate RAG in chatbots.

## Prerequisites

- Install `deepeval`
- A [Confident AI account](https://app.confident-ai.com) to get an API key

:::info
We recommend logging in to Confident AI to view your RAG evaluation report.

```bash
deepeval login
```

:::

## Evaluate RAG End-to-End

End-to-end RAG evaluation treats your entire LLM application as a standalone RAG pipeline. If your system contains multiple RAG pipelines, you'll need to [evaluate them as components](#evaluate-rag-as-a-component) instead.

<Timeline>
<TimelineItem title="Setup RAG pipeline">

Set up your RAG pipeline to return the retrieved contexts alongside the
LLM response.

<Tabs>
<TabItem value="python" label="Python">

```python title=main.py showLineNumbers={true} wrapLines={true}
def rag_pipeline(input):
   ...
   return 'RAG output', ['retrieved context 1', 'retrieved context 2', ...]
```

</TabItem>
<TabItem value="langgraph" label="LangGraph">

```python title="main.py" showLineNumbers={true} wrapLines={true}
from langchain_core.messages import HumanMessage
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.load_local("./faiss_index", embeddings)
retriever = vectorstore.as_retriever()
llm = ChatOpenAI(model="gpt-4")

def rag_pipeline(input):
    # Extract retrieval context
    retrieved_docs = retriever.get_relevant_documents(input)
    context_texts = [doc.page_content for doc in retrieved_docs]

    # Generate response
    state = {"messages": [HumanMessage(content=input + "\\n\\n".join(context_texts))]}
    result = your_graph.invoke(state)
    return result["messages"][-1].content, context_texts
```

</TabItem>
<TabItem value="langchain" label="LangChain">

```python title="main.py" showLineNumbers={true} wrapLines={true}
from langchain_openai import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

llm = ChatOpenAI(model="gpt-4")
vectorstore = Chroma(persist_directory="./chroma_db")
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

def rag_pipeline(input):
    # Extract retrieval context
    retrieved_docs = retriever.get_relevant_documents(input)
    context_texts = [doc.page_content for doc in retrieved_docs]

    # Generate response
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    result = qa_chain.invoke({"query": input})
    return result["result"], context_texts
```

</TabItem>
<TabItem value="llama_index" label="LlamaIndex">

```python title="main.py" showLineNumbers={true} wrapLines={true}
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

def rag_pipeline(input):
    # Generate response
    response = query_engine.query(input)

    # Extract retrieval context
    context_texts = []
    if hasattr(response, 'source_nodes'):
        context_texts = [node.text for node in response.source_nodes]
    return str(response), context_texts
```

</TabItem>
<TabItem value="pydantic" label="Pydantic AI">

```python title="main.py" showLineNumbers={true} wrapLines={true}
from pydantic_ai import Agent
from your_embedding_service import EmbeddingService

agent = Agent(
    model='openai:gpt-4o',
    system_prompt='Use the provided context to answer questions accurately.'
)
embedding_service = EmbeddingService()

def rag_pipeline(input):
    # Extract retrieval context
    retrieved_contexts = embedding_service.search_sync(input, top_k=3)
    context_texts = [ctx.text for ctx in retrieved_contexts]

    # Generate response
    result = agent.run_sync(input + '\n\n'.join(context_texts))
    return result.data, context_texts
```

</TabItem>
<TabItem value="openai-agents" label="OpenAI Agents">

```python title="main.py" showLineNumbers={true} wrapLines={true}
from agents import Agent, Runner
from your_vector_store import VectorStore

agent = Agent(
    name="RAG Assistant",
    instructions="You are a helpful assistant that uses provided context"
)
vector_store = VectorStore()

def rag_pipeline(input):
    # Extract retrieval context
    retrieved_docs = vector_store.similarity_search(input, k=3)
    context_texts = [doc.page_content for doc in retrieved_docs]

    # Generate response
    result = Runner.run_sync(agent, input + "\\n\\n".join(context_texts))
    return result.final_output, context_texts
```

</TabItem>
</Tabs>

:::tip
[Component-level evaluation](#evaluate-rag-as-a-component) doesn't require changing your pipeline's return types.
:::

</TimelineItem>
<TimelineItem title="Create a test case">

Create a test case using retrieval context and LLM output from your RAG pipeline. Optionally provide an expected output if you plan to use [contextual precision](/docs/metrics-contextual-precision) and [contextual recall](/docs/metrics-contextual-recall) metrics.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.test_case import LLMTestCase

input = 'What does YC look for?'
actual_output, retrieved_contexts = rag_pipeline(input)

test_case = LLMTestCase(
    input=input,
    actual_output=actual_output,
    retrieval_context=retrieved_contexts,
    expected_output='optional expected output'
)
```

</TimelineItem>
<TimelineItem title="Select metrics">

Select [RAG metrics](/docs/metrics-answer-relevancy) to evaluate your RAG pipeline, or define your own using [G-Eval](/docs/metrics-llm-evals).

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric

answer_relevancy = AnswerRelevancyMetric(threshold=0.8)
contextual_precision = ContextualPrecisionMetric(threshold=0.8)
```

<details>
<summary>What RAG metrics are available?</summary>

DeepEval offers a total of 5 RAG metrics, which are:

- [Answer Relevancy](/docs/metrics-answer-relevancy)
- [Faithfulness](/docs/metrics-faithfulness)
- [Contextual Relevancy](/docs/metrics-contextual-relevancy)
- [Contextual Precision](/docs/metrics-contextual-precision)
- [Contextual Recall](/docs/metrics-contextual-recall)

Each metric measures a [different parameter](/guides/guides-rag-evaluation) in your RAG pipeline's quality, and can help you determine the best prompts, models, and retriever settings for your use-case.

</details>

</TimelineItem>
<TimelineItem title="Running an evaluation">

Run an evaluation on the LLM test case you previously created using the metrics defined above.

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval import evaluate
...

evaluate([test_case], metrics=[answer_relevancy, contextual_precision])
```

</TimelineItem>

<TimelineItem title="Viewing on Confident AI">

If you're logged in, you'll be directed to [Confident AI](https://app.confident-ai.com) to view the evaluation report once the evaluation completes.

:::tip
We recommend logging in to Confident AI to view your RAG test report and benchmark your improvements.

```bash
deepeval login
```

:::

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Arag.mp4" />

</TimelineItem>
</Timeline>

## Evaluate RAG as a Component

DeepEval allows you to evaluate RAG components individually. This is useful if your system contains multiple RAG pipelines, such as Agentic RAG systems with access to multiple knowledge bases.

<Timeline>
<TimelineItem title="Setup tracing">

Attach the `@observe` decorator to functions/methods that make up your RAG system. These will represent individual components in your agent.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,3,6,10}"
from deepeval.tracing import observe, update_current_span

@observe()
def your_multi_rag_agent(input):

    @observe()
    def rag_component_1(input):
        return "rag pipeline output"

    @observe()
    def rag_component_2(input):
        return "rag pipeline output"

    return rag_component_1(input) + rag_component_2(input)
```

:::info important
Set the `CONFIDENT_TRACE_FLUSH=YES` in your CLI to prevent traces from being lost in case of an early program termination.

```bash
export CONFIDENT_TRACE_FLUSH=YES
```

:::

</TimelineItem>
<TimelineItem title="Define metrics">

Select [RAG metrics](/docs/metrics-answer-relevancy) to evaluate your RAG pipeline, or define your own using [G-Eval](/docs/metrics-llm-evals).

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{1,6}"
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

answer_relevancy = AnswerRelevancyMetric(threshold=0.8)
faithfulness = FaithfulnessMetric(threshold=0.8)
```

</TimelineItem>
<TimelineItem title="Configure metrics">

Supply the metrics to the `@observe` decorator of each function, then define a test case in `update_span` with the parameters required by the metrics defined above.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,7,11,12,13,1,22,23,24}"
from deepeval.tracing import observe, update_current_span
...

@observe()
def your_multi_rag_agent(input):

    @observe(metrics=[answer_relevancy, faithfulness])
    def rag_component_1(input):
        update_current_span(
          test_case=LLMTestCase(
            input=input,
            actual_output="rag pipeline output"
            retrieval_context=["context 1", "context 2"]
          )
        )
        return "rag pipeline output"

    @observe(metrics=[answer_relevancy, faithfulness])
    def rag_component_2(input):
        update_current_span(
          test_case=LLMTestCase(
            input=input,
            actual_output="rag pipeline output"
            retrieval_context=["context 3", "context 4"]
          )
        )
        return "rag pipeline output"

    return rag_component_1(input) + rag_component_2(input)
```

</TimelineItem>
<TimelineItem title="Run an evaluation">

Finally, use the `dataset` iterator to invoke your RAG system on a list of goldens.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,4}"
from deepeval.dataset import EvaluationDataset, Golden
...

dataset = EvaluationDataset(goldens=[Golden(input='This is a test query')])
for golden in dataset.evals_iterator():
    your_multi_rag_agent(golden.input)
```

</TimelineItem>

<TimelineItem title="View on Confident AI">

If you're logged in, `deepeval` will automatically generate a test report on [Confident AI](https://app.confident.ai), DeepEval's platform.

:::tip
We recommend logging in to Confident AI to view and debug your evaluation traces in detail:

```bash
deepeval login
```

:::

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aai-agent-evals%3Aend-to-end.mp4" />

</TimelineItem>
</Timeline>

## Evaluate RAG in Chatbot Systems
