---
id: getting-started-rag
title: Evaluating RAG
sidebar_label: RAG Evals
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

DeepEval makes it easy to evaluate any **RAG** pipelines and systems, including QA agents and customer support chatbots.

<details>
  <summary>Click to learn more about RAG evaluation</summary>
</details>

In this guide, you'll learn how to set up your RAG pipeline for evaluations in **under 5 minutes**:

1. Prepare test cases.
2. Evaluate your RAG pipeline.

:::info
**Log in to Confident AI** to view your RAG evaluation report:

```bash
deepeval login
```

:::

## Prepare Test Cases

<Timeline>
  <TimelineItem title="Setup RAG pipeline">
    <p>Set up your RAG pipeline to return the retrieved contexts alongside the LLM response.</p>
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
    >
      {"def rag_pipeline(input):\n" + 
        "   ...\n\n" +
        "   return 'RAG output', ['retrieved context 1', 'retrieved context 2', ...]"}
    </CodeBlock>

  </TimelineItem>
  <TimelineItem title="Create a test case">
    <p>Create a test case using the outputs of your newly set up RAG pipeline.</p>
   <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
      metastring="{1,4}"
    > 
      {"from deepeval.test_case import LLMTestCase\n" + 
        "\n" +
        "input = 'What does YC look for?'\n" +
        "actual_output, retrieved_contexts = rag_pipeline(input)\n" +
        "\n" +
        "test_case = LLMTestCase(\n" +
        "    input=input,\n" + 
        "    actual_output=actual_output,\n" +
        "    retrieved_contexts=retrieved_contexts\n" +
        ")"}
    </CodeBlock>
  </TimelineItem>
  <TimelineItem title="Fill test cases">
    <p>Optionally provide an expected output if you plan to run contextual precision and recall metrics.</p>
   <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
      metastring="{6}"
    > 
      {"from deepeval.test_case import LLMTestCase\n" + 
        "...\n\n" +
        "test_case = LLMTestCase(\n" +
        "    ...,\n" + 
        "    expected_output='expected output',\n" +
        ")"}
    </CodeBlock>
  </TimelineItem>
</Timeline>

## Running Evaluations

<Timeline>
  <TimelineItem title="Select metrics">
    <p>Select <a href="/docs/metrics-answer-relevancy">RAG metrics</a> to evaluate your RAG pipeline, or define your own using <a href="/docs/metrics-llm-evals">G-Eval</a>.</p>
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
    >
      {"from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric\n" +
        "\n" +
        "answer_relevancy = AnswerRelevancyMetric(threshold=0.8)\n" +
        "contextual_precision = ContextualPrecisionMetric(threshold=0.8)"}
    </CodeBlock>

  </TimelineItem>
  <TimelineItem title="Running an evaluation">
    <p>Run an evaluation on the LLM test case you previously created using the metrics defined above.</p>
   <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
    >
      {"from deepeval import evaluate\n" +
        "...\n\n" +
        "evaluate([test_case], metrics=[answer_relevancy, contextual_precision])"}
    </CodeBlock>
    <p>Once the evaluation completes, you'll be directed to Confident AI to view the evaluation report</p>
    <VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Arag-test-report.mp4" />
  </TimelineItem>
</Timeline>
