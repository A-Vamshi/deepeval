---
id: getting-started-rag
title: Evaluating RAG
sidebar_label: RAG Evals
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

DeepEval makes it easy to evaluate any **RAG** pipelines and systems, including QA agents and customer support chatbots.

<details>
  <summary>Click to learn more about RAG evaluation</summary>

[RAG Evaluation](/guides/guides-rag-evaluation) is one of the most popular use-cases for DeepEval. A good RAG evaluation pipeline can help you determine the best prompts, models, and retriever settings for your specific use-case.

DeepEval's RAG metrics require you to set up an `LLMTestCase`, which represents a single interaction with your RAG pipeline.

![ok](https://deepeval-docs.s3.amazonaws.com/docs:llm-test-case.png)

DeepEval offers a total of 5 RAG metrics, which are:

- [Answer Relevancy](/docs/metrics-answer-relevancy)
- [Faithfulness](/docs/metrics-faithfulness)
- [Contextual Relevancy](/docs/metrics-contextual-relevancy)
- [Contextual Precision](/docs/metrics-contextual-precision)
- [Contextual Recall](/docs/metrics-contextual-recall)

</details>

In this guide, you'll learn how to set up your RAG pipeline for evaluations in **under 5 minutes**:

1. Prepare test cases.
2. Evaluate your RAG pipeline.

:::info
**Log in to Confident AI** to view your RAG evaluation report:

```bash
deepeval login
```

:::

## Prepare Test Cases

<Timeline>
  <TimelineItem title="Setup RAG pipeline">
    <p>
      Set up your RAG pipeline to return the retrieved contexts alongside the
      LLM response.
    </p>
    <Tabs>
      <TabItem value="python" label="Python">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"def rag_pipeline(input):\n" +
            "   ...\n" +
            "   return 'RAG output', ['retrieved context 1', 'retrieved context 2', ...]"}
        </CodeBlock>
      </TabItem>
      <TabItem value="langgraph" label="LangGraph">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"from langchain_core.messages import HumanMessage\n" +
            "from langchain.vectorstores import FAISS\n" +
            "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n\n" +
            "embeddings = OpenAIEmbeddings()\n" +
            'vectorstore = FAISS.load_local("./faiss_index", embeddings)\n' +
            "retriever = vectorstore.as_retriever()\n" +
            'llm = ChatOpenAI(model="gpt-4")\n\n' +
            "def rag_pipeline(input):\n" +
            "    # Extract retrieval context\n" +
            "    retrieved_docs = retriever.get_relevant_documents(input)\n" +
            "    context_texts = [doc.page_content for doc in retrieved_docs]\n" +
            "    \n" +
            "    # Generate response\n" +
            '    state = {"messages": [HumanMessage(content=input + "\\n\\n".join(context_texts))]}\n' +
            "    result = your_graph.invoke(state)\n" +
            '    return result["messages"][-1].content, context_texts'}
        </CodeBlock>
      </TabItem>
      <TabItem value="langchain" label="LangChain">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"from langchain_openai import ChatOpenAI\n" +
            "from langchain.vectorstores import Chroma\n" +
            "from langchain.chains import RetrievalQA\n\n" +
            'llm = ChatOpenAI(model="gpt-4")\n' +
            'vectorstore = Chroma(persist_directory="./chroma_db")\n' +
            'retriever = vectorstore.as_retriever(search_kwargs={"k": 3})\n\n' +
            "def rag_pipeline(input):\n" +
            "    # Extract retrieval context\n" +
            "    retrieved_docs = retriever.get_relevant_documents(input)\n" +
            "    context_texts = [doc.page_content for doc in retrieved_docs]\n" +
            "    \n" +
            "    # Generate response\n" +
            "    qa_chain = RetrievalQA.from_chain_type(\n" +
            "        llm=llm,\n" +
            '        chain_type="stuff",\n' +
            "        retriever=retriever,\n" +
            "        return_source_documents=True\n" +
            "    )\n" +
            '    result = qa_chain.invoke({"query": input})\n' +
            '    return result["result"], context_texts'}
        </CodeBlock>
      </TabItem>
      <TabItem value="llama_index" label="LlamaIndex">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n" +
            'documents = SimpleDirectoryReader("./data").load_data()\n' +
            "index = VectorStoreIndex.from_documents(documents)\n" +
            "query_engine = index.as_query_engine()\n\n" +
            "def rag_pipeline(input):\n" +
            "    # Generate response\n" +
            "    response = query_engine.query(input)\n" +
            "    \n" +
            "    # Extract retrieval context\n" +
            "    context_texts = []\n" +
            "    if hasattr(response, 'source_nodes'):\n" +
            "        context_texts = [node.text for node in response.source_nodes]\n" +
            "    return str(response), context_texts"}
        </CodeBlock>
      </TabItem>
      <TabItem value="pydantic" label="Pydantic AI">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"from pydantic_ai import Agent\n" +
            "from your_embedding_service import EmbeddingService\n\n" +
            "agent = Agent(\n" +
            "    model='openai:gpt-4o',\n" +
            "    system_prompt='Use the provided context to answer questions accurately.'\n" +
            ")\n" +
            "embedding_service = EmbeddingService()\n\n" +
            "def rag_pipeline(input):\n" +
            "    # Extract retrieval context\n" +
            "    retrieved_contexts = embedding_service.search_sync(input, top_k=3)\n" +
            "    context_texts = [ctx.text for ctx in retrieved_contexts]\n\n" +
            "    # Generate response\n" +
            "    result = agent.run_sync(input + '\\n\\n'.join(context_texts))\n" +
            "    return result.data, context_texts"}
        </CodeBlock>
      </TabItem>
      <TabItem value="openai-agents" label="OpenAI Agents">
        <CodeBlock
          language="python"
          title="main.py"
          showLineNumbers={true}
          wrapLines={true}
        >
          {"from agents import Agent, Runner\n" +
            "from your_vector_store import VectorStore\n\n" +
            "agent = Agent(\n" +
            '    name="RAG Assistant",\n' +
            '    instructions="You are a helpful assistant that uses provided context"\n' +
            ")\n" +
            "vector_store = VectorStore()\n\n" +
            "def rag_pipeline(input):\n" +
            "    # Extract retrieval context\n" +
            "    retrieved_docs = vector_store.similarity_search(input, k=3)\n" +
            "    context_texts = [doc.page_content for doc in retrieved_docs]\n" +
            "    \n" +
            "    # Generate response\n" +
            '    result = Runner.run_sync(agent, input + "\\n\\n".join(context_texts))\n' +
            "    return result.final_output, context_texts"}
        </CodeBlock>
      </TabItem>
      <TabItem value="crewai" label="CrewAI">
        <p>
          <i>To be documented...</i>
        </p>
      </TabItem>
    </Tabs>
  </TimelineItem>
  <TimelineItem title="Create a test case">
    <p>
      Create a test case using the outputs of your newly set up RAG pipeline.
    </p>
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
      metastring="{1,4}"
    >
      {"from deepeval.test_case import LLMTestCase\n" +
        "\n" +
        "input = 'What does YC look for?'\n" +
        "actual_output, retrieved_contexts = rag_pipeline(input)\n" +
        "\n" +
        "test_case = LLMTestCase(\n" +
        "    input=input,\n" +
        "    actual_output=actual_output,\n" +
        "    retrieved_contexts=retrieved_contexts\n" +
        ")"}
    </CodeBlock>
  </TimelineItem>
  <TimelineItem title="Fill test cases">
    <p>
      Optionally provide an expected output if you plan to run contextual
      precision and recall metrics.
    </p>
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
      metastring="{6}"
    >
      {"from deepeval.test_case import LLMTestCase\n" +
        "...\n\n" +
        "test_case = LLMTestCase(\n" +
        "    ...,\n" +
        "    expected_output='expected output',\n" +
        ")"}
    </CodeBlock>
  </TimelineItem>
</Timeline>

## Running Evaluations

<Timeline>
  <TimelineItem title="Select metrics">
    <p>Select <a href="/docs/metrics-answer-relevancy">RAG metrics</a> to evaluate your RAG pipeline, or define your own using <a href="/docs/metrics-llm-evals">G-Eval</a>.</p>
    <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
    >
      {"from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric\n" +
        "\n" +
        "answer_relevancy = AnswerRelevancyMetric(threshold=0.8)\n" +
        "contextual_precision = ContextualPrecisionMetric(threshold=0.8)"}
    </CodeBlock>

  </TimelineItem>
  <TimelineItem title="Running an evaluation">
    <p>Run an evaluation on the LLM test case you previously created using the metrics defined above.</p>
   <CodeBlock
      language="python"
      title="main.py"
      showLineNumbers={true}
      wrapLines={true}
    >
      {"from deepeval import evaluate\n" +
        "...\n\n" +
        "evaluate([test_case], metrics=[answer_relevancy, contextual_precision])"}
    </CodeBlock>
    <p>Once the evaluation completes, you'll be directed to Confident AI to view the evaluation report</p>
    <VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Arag-test-report.mp4" />
  </TimelineItem>
</Timeline>
