---
id: metrics-exact-match
title: Exact Match
sidebar_label: Exact Match
---

<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/metrics-exact-match"
  />
</head>

import Equation from "@site/src/components/Equation";
import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer singleTurn={true} usesLLMs={false} referenceless={false} />

The Exact Match metric measures whether your LLM application's `actual_output` exactly matches the `expected_output`. If the outputs are not identical, it optionally computes an **F1-based similarity** score using token overlap.

:::note
The `ExactMatchMetric` does **not** rely on an LLM for evaluation. It purely compares the outputs on a lexical level (string or token-level comparison).
:::


## Required Arguments

To use the `ExactMatchMetric`, you'll have to provide the following arguments when creating an [`LLMTestCase`](/docs/evaluation-test-cases#llm-test-case):

- `input`
- `actual_output`
- `expected_output`

Read the [How Is It Calculated](#how-is-it-calculated) section below to learn how test case parameters are used for metric calculation.

## Usage

```python
from deepeval import evaluate
from deepeval.metrics import ExactMatchMetric
from deepeval.test_case import LLMTestCase

metric = ExactMatchMetric(
    compute_f1=True,
    threshold=0.5,
    verbose_mode=True,
)

test_case = LLMTestCase(
    input="Translate 'Hello, how are you?' in french",
    actual_output="Bonjour, comment Ã§a va ?",
    expected_output="Bonjour, comment allez-vous ?"
)

# To run metric as a standalone
# metric.measure(test_case)
# print(metric.score, metric.reason)

evaluate(test_cases=[test_case], metrics=[metric])
```

There are **FIVE** optional parameters when creating an `ExactMatchMetric`:

- [Optional] `compute_f1`: a boolean when set to `True`, computes **precision**, **recall**, and **F1** based on token overlap if the outputs are not exact matches. Defaulted to `True`.
- [Optional] `threshold`: a float representing the minimum passing threshold, defaulted to 0.5.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.

### As a Standalone

You can also run the `ExactMatchMetric` on a single test case as a standalone, one-off execution.

```python
...

metric.measure(test_case)
print(metric.score, metric.reason)
```

## How Is It Calculated?

The `ExactMatchMetric` score is calculated according to the following equations:

<Equation
formula="\text{Exact Match Score} =
    \begin{cases}
    1 & \text{if actual\_output = expected\_output}, \\
    \text{F1(actual, expected)} & \text{(if compute\_f1=True)}, \\
    0 & \text{(if compute\_f1=False)}
    \end{cases}"
/>

Where:

<Equation formula="\text{Precision} = \frac{\text{Num of overlapping tokens}}{\text{Num of tokens in actual output}}" />

<Equation formula="\text{Recall} = \frac{\text{Num of overlapping tokens}}{\text{Num of tokens in expected output}}" />

<Equation formula="\text{F1} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}" />

The `ExactMatchMetric` does not use an LLM for evaluation and instead uses the provided `expected_output` to find the exact match score of `actual_output`.
