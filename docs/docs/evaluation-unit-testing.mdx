---
id: evaluation-unit-testing-in-ci-cd
title: Unit Testing in CI/CD
sidebar_label: Unit Testing in CI/CD
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import VideoDisplayer from "@site/src/components/VideoDisplayer";

Integrate LLM evaluations into your CI/CD pipeline with `deepeval` to catch regressions and ensure reliable performance. You can use `deepeval` with your CI/CD pipelines to run both end-to-end and component level evaluations.

`deepeval` allows you to run evaluations as if you're using `pytest` via our Pytest integration.

## End-to-End Evals in CI/CD

Run tests against your LLM app using golden datasets for every push you make. End-to-end evaluations validate overall behavior across single-turn and multi-turn interactions. Perfect for catching regressions before deploying to production.

### Single-Turn Evals

<Timeline>
<TimelineItem title="Pull your dataset">

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="Your Dataset Name") # replace with your alias
```

You can use this dataset to iterate over goldens.

</TimelineItem>
<TimelineItem title="Create your pytest file">

```python title="test_llm_app.py" showLineNumbers {14}
from your_agent import your_llm_app # Replace with your LLM app
import pytest

from deepeval.dataset import Golden
from deepeval.test_case import LLMTestCase
from deepeval import assert_test

# Loop through goldens using pytest

@pytest.mark.parametrize("golden",dataset.goldens)
def test_llm_app(golden: Golden):
    res, text_chunks = your_llm_app(golden.input)
    test_case = LLMTestCase(input=golden.input, actual_output=res, retrieval_context=text_chunks)
    assert_test(test_case=test_case, metrics=[AnswerRelevancyMetric()])

@deepeval.log_hyperparameters(model="gpt-4", prompt_template="...")
def hyperparameters():
    return {"model": "gpt-4.1", "system prompt": "..."}
```

Then, run the following command in your CLI:

```bash
deepeval test run test_llm_app.py
```

There are **TWO** mandatory and **ONE** optional parameter when calling the `assert_test()` function for **END-TO-END** evaluation:

- `test_case`: an `LLMTestCase`.
- `metrics`: a list of metrics of type `BaseMetric`.
- [Optional] `run_async`: a boolean which when set to `True`, enables concurrent evaluation of all metrics in `@observe`. Defaulted to `True`.


</TimelineItem>
<TimelineItem title="Create a YAML file to test on every push">

You just need to create a YAML file now to test your LLM app on every change you make.

```yaml {32-33}
name: LLM App DeepEval Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: poetry install --no-root

      - name: Run DeepEval Unit Tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: poetry run deepeval test run test_llm_app.py
```
</TimelineItem>
</Timeline>


### Multi-Turn Evals

<Timeline>
<TimelineItem title="Pull your dataset">

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="Multi-Turn Dataset") # replace with your alias
```

You can use this dataset to iterate over goldens. You also need to create a callback function that allows you to simulate conversations and create test cases. 

Click here to see [how to create a callback function for your chatbot](/docs/evaluation-end-to-end-llm-evals#multi-turn-end-to-end-evals).

</TimelineItem>
<TimelineItem title="Create your pytest file">

```python title="test_llm_app.py" showLineNumbers {14}
from main import chatbot_callback # Replace with your LLM callback
import pytest

from deepeval.dataset import Golden
from deepeval.test_case import ConversationalTestCase
from deepeval import assert_test
from deepeval.conversation_simulator import ConversationSimulator

# Loop through goldens using pytest

simulator = ConversationSimulator(model_callback=chatbot_callback)
conversational_test_cases = simulator.simulate(goldens=dataset.goldens, max_turns=10)

@pytest.mark.parametrize("test_case", conversational_test_cases)
def test_llm_app(test_case: ConversationalTestCase):
    assert_test(test_case=test_case, metrics=[AnswerRelevancyMetric()])

@deepeval.log_hyperparameters(model="gpt-4", prompt_template="...")
def hyperparameters():
    return {"model": "gpt-4.1", "system prompt": "..."}
```

Then, run the following command in your CLI:

```bash
deepeval test run test_llm_app.py
```

There are **TWO** mandatory and **ONE** optional parameter when calling the `assert_test()` function for **END-TO-END** evaluation:

- `test_case`: an `LLMTestCase`.
- `metrics`: a list of metrics of type `BaseMetric`.
- [Optional] `run_async`: a boolean which when set to `True`, enables concurrent evaluation of all metrics in `@observe`. Defaulted to `True`.


</TimelineItem>
<TimelineItem title="Create a YAML file to test on every push">

You just need to create a YAML file now to test your LLM app on every change you make.

```yaml {32-33}
name: LLM App DeepEval Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: poetry install --no-root

      - name: Run DeepEval Unit Tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: poetry run deepeval test run test_llm_app.py
```
</TimelineItem>
</Timeline>

:::caution
The usual `pytest` command would still work but is highly not recommended. `deepeval test run` adds a range of functionalities on top of Pytest for unit-testing LLMs, which is enabled by [8+ optional flags](/docs/evaluation-flags-and-configs#flags-for-deepeval-test-run). Users typically include `deepeval test run` as a command in their `.yaml` files for pre-deployment checks in CI/CD pipelines ([example here](https://www.confident-ai.com/docs/llm-evaluation/unit-testing-cicd)).
:::

[Click here](/docs/evaluation-flags-and-configs#flags-for-deepeval-test-run) to learn about different optional flags available to `deepeval test run` to customize asynchronous behaviors, error handling, etc.


## Component-Level Evals in CI/CD

Test individual parts of your LLM pipeline like prompt templates or retrieval logic in isolation. Component-level evals offer fast, targeted feedback and integrate seamlessly into your CI/CD workflows.

<Timeline>
<TimelineItem title="Pull your dataset">

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="Your Dataset Name") # replace with your alias
```

You can use this dataset to iterate over goldens.

</TimelineItem>
<TimelineItem title="Create your pytest file">

```python title="test_llm_app.py" {10}
import pytest
from your_agent import your_llm_app # Replace with your LLM app

from deepeval import assert_test
from deepeval.dataset import Golden

# Loop through goldens in our dataset using pytest
@pytest.mark.parametrize("golden", dataset.goldens)
def test_llm_app(golden: Golden):
    assert_test(golden=golden, observed_callback=your_llm_app)

@deepeval.log_hyperparameters(model="gpt-4", prompt_template="...")
def hyperparameters():
    return {"model": "gpt-4.1", "system prompt": "..."}
```

Finally, don't forget to run the test file in the CLI:

```bash
deepeval test run test_llm_app.py
```

There are **TWO** mandatory and **ONE** optional parameter when calling the `assert_test()` function for **COMPONENT-LEVEL** evaluation:

- `golden`: the `Golden` that you wish to invoke your `observed_callback` with.
- `observed_callback`: a function callback that is your `@observe` decorated LLM application. There must be **AT LEAST ONE** metric within one of the `metrics` in your `@observe` decorated LLM application.
- [Optional] `run_async`: a boolean which when set to `True`, enables concurrent evaluation of all metrics in `@observe`. Defaulted to `True`.

</TimelineItem>
<TimelineItem title="Create a YAML file to test on every push">

You just need to create a YAML file now to test your LLM app on every change you make.

```yaml {32-33}
name: LLM App DeepEval Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: poetry install --no-root

      - name: Run DeepEval Unit Tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: poetry run deepeval test run test_llm_app.py
```
</TimelineItem>
</Timeline>

:::info
Similar to the `evaluate()` function, `assert_test()` for component-level evaluation does not need:

- Declaration of `metrics` because those are defined at the span level in the `metrics` parameter.
- Creation of `LLMTestCase`s because it is handled at runtime by `update_current_span` in your LLM app.

:::

[Click here](/docs/evaluation-flags-and-configs#flags-for-deepeval-test-run) to learn about different optional flags available to `deepeval test run` to customize asynchronous behaviors, error handling, etc.

:::tip
We highly recommend setting up [Confident AI](https://app.confident-ai.com) with your `deepeval` evaluations to get professional test reports and observe trends of your LLM application's performance overtime like this:

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:spans.mp4"
  confidentUrl="/docs/llm-tracing/introduction"
  label="Span-Level Evals in Production"
/>
:::