---
id: getting-started-chatbots
title: LLM Evals for Chatbots
sidebar_label: Chatbots
---

import { Timeline, TimelineItem } from "@site/src/components/Timeline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import VideoDisplay from "@site/src/components/VideoDisplayer";

Learn to evaluate any type of **chatbot**, including QA agents, customer support chatbots, and even chatrooms.

## Overview

Chatbot Evaluation is different from other types of evaluations because unlike single-turn tasks, conversations happen over multiple turns. This means your chatbot must stay context-aware across the conversation, and not just accurate in individual responses. Conversations on DeepEval are captured by a <code>ConversationalTestCase</code>, which consist of multiple turns as shown below.

![Chatbot Evaluation](https://deepeval-docs.s3.amazonaws.com/docs:conversational-test-case.png)

**In this 5 min quickstart, you'll learn how to:**

1. Prepare conversational test cases.
2. Evaluate chatbot conversations.
3. Simulate conversations.

## Prerequisites

- Install `deepeval`
- A [Confident AI account](https://app.confident-ai.com) to get an API key

:::info
We recommend logging in to Confident AI to view your chatbot evaluation report and manage multi-turn datasets:

```bash
deepeval login
```

:::

## Evaluate Chatbot from Existing Conversations

If you don't have access to existing conversations, you'll want to evaluate your chatbot from [simulated conversations](#evaluate-chatbots-from-simulations) instead.

<Timeline>
<TimelineItem title="Create a test case">

Create a `ConversationalTestCase` by passing in a list of `Turn`s from an existing conversation, similar to OpenAI's message format.

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.test_case import ConversationalTestCase, Turn

test_case = ConversationalTestCase(
    turns=[
        Turn(role="user", content="Hello, how are you?"),
        Turn(role="assistant", content="I'm doing well, thank you!"),
        Turn(role="user", content="How can I help you today?"),
        Turn(role="assistant", content="I'd like to buy a ticket to a Coldplay concert."),
    ]
)
```

</TimelineItem>
<TimelineItem title="Run an evaluation">

Run an evaluation on the test case using `deepeval`'s [multi-turn metrics](/docs/metrics-conversational-g-eval), or create your own using [Conversational G-Eval](/docs/metrics-conversational-g-eval).

<details>
  <summary>What multi-turn metrics are available?</summary>

Conversational test cases can only be evaluated using multi-turn metrics, which include:

- [Conversational G-Eval](/docs/metrics-conversational-g-eval)
- [Role Adherence](/docs/metrics-role-adherence)
- [Knowledge Retention](/docs/metrics-knowledge-retention)
- [Conversation Completeness](/docs/metrics-conversation-completeness)
- [Conversation Relevancy](/docs/metrics-conversation-relevancy)

</details>

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.metrics import ConversationCompletenessMetric, KnowledgeRetentionMetric
from deepeval import evaluate
...

evaluate([test_case], metrics=[ConversationCompletenessMetric(), KnowledgeRetentionMetric()])
```

</TimelineItem>
<TimelineItem title="View on Confident AI">

If you're logged in to Confident AI, you'll be automatically directed to view your chatbot evaluation report.

:::tip
We recommend logging in to Confident AI to analyze your chatbot performance in detail.

```bash
deepeval login
```

:::

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aconversation-test-report.mp4" />

</TimelineItem>
</Timeline>

## Evaluate Chatbots from Simulations

Evaluating your chatbot from [simulated conversations](/docs/getting-started-chatbots#evaluate-chatbots-from-simulations) is the recommended approach for chatbot evaluation, because it allows you to quickly test your chatbot in any scenario, including edge cases.

<Timeline>
<TimelineItem title="Create goldens">

Create a `ConversationalGolden` by providing your user description, scenario, and expected outcome, for the conversation you wish to simulate.

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.dataset import ConversationalGolden

conversation_golden = ConversationalGolden(
    scenario="Andy Byron wants to purchase a VIP ticket to a Coldplay concert.",
    expected_outcome="Successful purchase of a ticket.",
    user_description="Andy Byron is the CEO of Astronomer.",
)
```

<details>
  <summary>Curating goldens in bulk</summary>

One `ConversationalGolden` will generate a single conversation. The quality and amount of details in the descriptions you provide to the golden directly impacts the quality a simulated conversation.

We recommend using [Confident AI's dataset editor](https://app.confident-ai.com) to curate goldens in bulk, which allows you to edit, annotate, and assign golden creation tasks to your team members.

<VideoDisplay src="http://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Achatbot-evals%3Amultiturn-dataset.mp4" />

</details>

</TimelineItem>
<TimelineItem title="Setup chatbot">

Define a callback function to generate the **next chatbot response** in a conversation, given the conversation history.

:::info
Your model callback should accept an `input`, and optionally `turns` and `thread_id`. It should return a `Turn` object.
:::

<Tabs>
<TabItem value="python" label="Python">

```python title="main.py" showLineNumbers={true} wrapLines={true}"
from deepeval.test_case import Turn

async def model_callback(input: str, turns: List[Turn], thread_id: str) -> Turn:
    # Replace with your chatbot
    response = await your_chatbot(input, turns, thread_id)
    return Turn(role="assistant", content=response)
```

</TabItem>
<TabItem value="openai" label="OpenAI">

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{6}"
from deepeval.test_case import Turn
from openai import OpenAI

client = OpenAI()

async def model_callback(input: str, turns: List[Turn]) -> str:
    messages = [
        {"role": "system", "content": "You are a ticket purchasing assistant"},
        *[{"role": t.role, "content": t.content} for t in turns],
        {"role": "user", "content": input},
    ]
    response = await client.chat.completions.create(model="gpt-4.1", messages=messages)
    return Turn(role="assistant", content=response.choices[0].message.content)
```

</TabItem>
<TabItem value="langchain" label="LangChain">

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{11}"
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

store = {}
llm = ChatOpenAI(model="gpt-4")
prompt = ChatPromptTemplate.from_messages([("system", "You are a ticket purchasing assistant."), MessagesPlaceholder(variable_name="history"), ("human", "{input}")])
chain_with_history = RunnableWithMessageHistory(prompt | llm, lambda session_id: store.setdefault(session_id, ChatMessageHistory()), input_messages_key="input", history_messages_key="history")

async def model_callback(input: str, thread_id: str) -> Turn:
    response = chain_with_history.invoke(
        {"input": input},
        config={"configurable": {"session_id": thread_id}}
    )
    return Turn(role="assistant", content=response.content)
```

</TabItem>
<TabItem value="llama_index" label="LlamaIndex">

```python title="main.py"  showLineNumbers={true} wrapLines={true} metastring="{9}"
from llama_index.core.storage.chat_store import SimpleChatStore
from llama_index.llms.openai import OpenAI
from llama_index.core.chat_engine import SimpleChatEngine
from llama_index.core.memory import ChatMemoryBuffer

chat_store = SimpleChatStore()
llm = OpenAI(model="gpt-4")

async def model_callback(input: str, thread_id: str) -> Turn:
    memory = ChatMemoryBuffer.from_defaults(chat_store=chat_store, chat_store_key=thread_id)
    chat_engine = SimpleChatEngine.from_defaults(llm=llm, memory=memory)
    response = chat_engine.chat(input)
    return Turn(role="assistant", content=response.response)
```

</TabItem>
<TabItem value="openai-agents" label="OpenAI Agents">

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{6}"
from agents import Agent, Runner, SQLiteSession

sessions = {}
agent = Agent(name="Test Assistant", instructions="You are a helpful assistant that answers questions concisely.")

async def model_callback(input: str, thread_id: str) -> Turn:
    if thread_id not in sessions:
        sessions[thread_id] = SQLiteSession(thread_id)
    session = sessions[thread_id]
    result = await Runner.run(agent, input, session=session)
    return Turn(role="assistant", content=result.final_output)
```

</TabItem>
<TabItem value="pydantic" label="Pydantic">

```python title="main.py" showLineNumbers={true} wrapLines={true} metastring="{9}"
from pydantic_ai.messages import ModelRequest, ModelResponse, UserPromptPart, TextPart
from deepeval.test_case import Turn
from datetime import datetime
from pydantic_ai import Agent
from typing import List

agent = Agent('openai:gpt-4', system_prompt="You are a helpful assistant that answers questions concisely.")

async def model_callback(input: str, turns: List[Turn]) -> Turn:
    message_history = []
    for turn in turns:
        if turn.role == "user":
            message_history.append(ModelRequest(parts=[UserPromptPart(content=turn.content, timestamp=datetime.now())], kind='request'))
        elif turn.role == "assistant":
            message_history.append(ModelResponse(parts=[TextPart(content=turn.content)], model_name='gpt-4', timestamp=datetime.now(), kind='response'))
    result = await agent.run(input, message_history=message_history)
    return Turn(role="assistant", content=result.output)
```

</TabItem>
</Tabs>

</TimelineItem>
<TimelineItem title="Simulate conversations">

Create a [conversation simulator](/docs/conversation-simulator) with the model callback, and run simulations to generate conversational test cases from the conversational golden you created.

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.conversation_simulator import ConversationSimulator

simulator = ConversationSimulator(model_callback=chatbot_callback)
conversational_test_cases = simulator.simulate(
    goldens=[conversation_golden],
    max_turns=10,
)
```

<details>
<summary>Click to view an example simulated test case</summary>

Your generated test cases should be populated with simulated `Turn`s, along with the `scenario`, `expected_outcome`, and `user_description` from the conversation golden.

```python
ConversationalTestCase(
    scenario="Andy Byron wants to purchase a VIP ticket to a Coldplay concert.",
    expected_outcome="Successful purchase of a ticket.",
    user_description="Andy Byron is the CEO of Astronomer.",
    turns=[
        Turn(role="user", content="Hello, how are you?"),
        Turn(role="assistant", content="I'm doing well, thank you!"),
        Turn(role="user", content="How can I help you today?"),
        Turn(role="assistant", content="I'd like to buy a ticket to a Coldplay concert."),
    ]
)
```

</details>

</TimelineItem>

<TimelineItem title="Run an evaluation">

Run an evaluation on the simulated test cases using `deepeval`'s [multi-turn metrics](/docs/metrics-conversational-g-eval).

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.metrics import ConversationRelevancyMetric
from deepeval import evaluate

evaluate(conversational_test_cases, metrics=[ConversationRelevancyMetric()])
```

</TimelineItem>

<TimelineItem title="View on Confident AI">

If you're logged in to Confident AI, you'll be automatically directed to view your chatbot evaluation report.

:::tip
We recommend logging in to Confident AI to analyze your chatbot performance in detail.

```bash
deepeval login
```

:::

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aconversation-test-report.mp4" />

</TimelineItem>
</Timeline>

## Evaluate Chatbots in Production

`deepeval` allows you to evaluate your chatbot conversations from production as well, but you'll need a [Confident AI](https://app.confident-ai.com) account to trace and store these conversations to run offline evaluations.

<Timeline>
<TimelineItem title="Setup tracing">

Attach the `@observe` decorator to your chatbot's outermost function, and record the user input, chatbot response, and thread ID in `update_current_trace`.

```python title=main.py showLineNumbers={true} wrapLines={true} metastring="{1,3,7,8,9}"
from deepeval.tracing import observe, update_current_trace

@observe()
async def chatbot(input):
    response = f"Chatbot response to: {input}"
    update_current_trace(
        input=input,
        output=response,
        thread_id="thread_id",
    )
    return response
```

If you're running your chatbot locally, set the `CONFIDENT_TRACE_FLUSH` environment variable to `YES` to enable tracing.

```bash
export CONFIDENT_TRACE_FLUSH=YES
```

</TimelineItem>
<TimelineItem title="Invoke your chatbot">

Talk to your chatbot as you normally would. If you're logged in, all conversations will be automatically traced and sent to [Confident AI](https://app.confident-ai.com).

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Achatbot-evals%3Athreads.mp4" />

:::note

You should see a trace log like the one below in your CLI for each conversation turn, if tracing is successfully set up.

<pre>
  <code>
    <span
      style={{ color: "#7f7f7f", fontWeight: "bold", whiteSpace: "nowrap" }}
    >
      [Confident AI Trace Log]{"  "}
    </span>
    <span style={{ color: "#00ff00", whiteSpace: "nowrap" }}>
      Successfully posted trace (...):{" "}
    </span>
    <span
      style={{
        color: "#5f5fff",
        textDecoration: "underline",
        whiteSpace: "nowrap",
      }}
    >
      https://app.confident.ai/[...]
    </span>
  </code>
</pre>

:::

</TimelineItem>
<TimelineItem title="Create metric collection">

Navigate to [Confident AI](https://app.confident-ai.com) and create a [metric collection](https://documentation.confident-ai.com/docs/llm-evaluation/metrics/create-on-the-cloud#create-a-metric-collection) with the multi-turn metrics you wish to use.

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Achatbot-evals%3Ametric-collection.mp4" />

</TimelineItem>
<TimelineItem title="Run an evaluation">

Find the thread ID of the conversation you wish to evaluate and run an evaluation using the metric collection you created.

```python title="main.py" showLineNumbers={true} wrapLines={true}
from deepeval.tracing import evaluate_thread

evaluate_thread(thread_id="your_thread_id", metric_collection="Collection Name")
```

If you're logged in to [Confident AI](https://app.confident-ai.com), you can view these evaluations in the [Evaluations](https://app.confident-ai.com/evaluations) tab of each thread.

<VideoDisplay src="https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Achatbot-evals%3Aoffline-evals.mp4" />

</TimelineItem>

</Timeline>
