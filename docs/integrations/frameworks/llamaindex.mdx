---
id: llamaindex
title: LlamaIndex
sidebar_label: LlamaIndex
---


import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { Timeline, TimelineItem } from "@site/src/components/Timeline";

# LlamaIndex

DeepEval makes it easy to evaluate [LlamaIndex](https://www.llamaindex.ai/) applications in both development and production environments.

:::tip
We recommend logging in to [Confident AI](https://app.confident-ai.com) to view your LlamaIndex evaluation traces.

```bash
deepeval login
```

:::

## End-to-End Evals

DeepEval allows you to evaluate LlamaIndex applications end-to-end in **under a minute**.

<Timeline>

<TimelineItem title="Configure LlamaIndex">

Create a `FunctionAgent` with a list of [task completion metrics](/docs/metrics-task-completion) you wish to use, and pass it to your LlamaIndex application's `run` method.

```python title="main.py" showLineNumbers {2,15}
from llama_index.llms.openai import OpenAI
import llama_index.core.instrumentation as instrument

from deepeval.integrations.llama_index import instrument_llama_index
from deepeval.integrations.llama_index import FunctionAgent

def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b

answer_relevancy_metric = AnswerRelevancyMetric()
agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
    metrics=[answer_relevancy_metric],
)

agent.run("What is 3 * 12?")
```

:::info
Offline evaluation is supported for LlamaIndex `FunctionAgent`, `ReActAgent` and `CodeActAgent`. And only metrics with parameters `input` and `output` are supported for evaluation.
:::

</TimelineItem>
<TimelineItem title="Run evaluations">

Create an `EvaluationDataset` and invoke your LlamaIndex application for each golden within the `evals_iterator()` loop to run end-to-end evaluations.

<Tabs groupId="langgraph">
<TabItem value="synchronous" label="Synchronous">

```python title="main.py" showLineNumbers {5}
from deepeval.dataset import EvaluationDataset, Golden
...

dataset = EvaluationDataset(goldens=[Golden(input="What is 3 * 12?")])
for golden in dataset.evals_iterator():
    agent.run_sync(golden.input)
```

</TabItem>
  <TabItem value="asynchronous" label="Asynchronous">

```python title="main.py" showLineNumbers
from deepeval.dataset import EvaluationDataset, Golden
import asyncio
...

async def llm_app(input: str):
    await agent.run(input)

dataset = EvaluationDataset(goldens=[
    Golden(input="What's 7 * 8?"),
    Golden(input="What's 7 * 6?")
])

for golden in dataset.evals_iterator():
    task = asyncio.create_task(llm_app(golden.input))
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

âœ… Done. The `evals_iterator` will automatically generate a test run with individual evaluation traces for each golden.

</TimelineItem>

</Timeline>

:::note
If you need to evaluate individual components of your LlamaIndex application, [set up tracing](/docs/evaluation-llm-tracing) instead.
:::

## Evals in Production

To run online evaluations in production, simply replace `metrics` in `FunctionAgent` with a [metric collection](https://documentation.confident-ai.com/docs/llm-tracing/evaluations#online-evaluations) string from Confident AI, and push your LlamaIndex agent to production.

```python filename="main.py" showLineNumbers {8}
from deepeval.integrations.llama_index import FunctionAgent
...

# Invoke your agent with the metric collection name
agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
    metric_collection="test_collection_1",
)

agent.run("What is 3 * 12?")
```
