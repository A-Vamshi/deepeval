---
id: langchain
title: LangChain
sidebar_label: LangChain
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { Timeline, TimelineItem } from "@site/src/components/Timeline";

# LangChain

DeepEval makes it easy to evaluate [LangChain](https://www.langchain.com/) applications in both development and production environments.

:::tip
We recommend logging in to [Confident AI](https://app.confident-ai.com) to view your LangChain evaluation traces.

```bash
deepeval login
```

:::

## End-to-End Evals

DeepEval allows you to evaluate LangChain applications end-to-end in **under a minute**.

<Timeline>

<TimelineItem title="Configure LangChain">

Create a `CallbackHandler` with a list of [task completion metrics](/docs/metrics-task-completion) you wish to use, and pass it to your LangChain application's `invoke` method.

```python title="main.py" showLineNumbers {2,15}
from langchain.chat_models import init_chat_model
from deepeval.integrations.langchain import CallbackHandler
from deepeval.metrics import TaskCompletionMetric

def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
llm_with_tools = llm.bind_tools([multiply])

# Create goldens
llm_with_tools.invoke(
    "What is 3 * 12?",
    config = {"callbacks": [CallbackHandler(metrics=[TaskCompletionMetric(task="multiplication")])]}
)
```

:::info
Only [Task Completion](/docs/metrics-task-completion) is supported for the LangChain integration. To use other metrics, manually [set up tracing](/docs/evaluation-llm-tracing) instead.
:::

</TimelineItem>
<TimelineItem title="Run evaluations">

Create an `EvaluationDataset` and invoke your LangChain application for each golden within the `evals_iterator()` loop to run end-to-end evaluations.

<Tabs groupId="langgraph">
<TabItem value="synchronous" label="Synchronous">

```python title="main.py" showLineNumbers {5}
from deepeval.dataset import EvaluationDataset, Golden
...

dataset = EvaluationDataset(goldens=[Golden(input="What is 3 * 12?")])
for golden in dataset.evals_iterator():
    llm_with_tools.invoke(
        "What is 3 * 12?",
        config = {"callbacks": [CallbackHandler(metrics=[TaskCompletionMetric()])]}
    )
```

</TabItem>
  <TabItem value="asynchronous" label="Asynchronous">

```python title="main.py" showLineNumbers {6}
from deepeval.dataset import EvaluationDataset, Golden
import asyncio
...

dataset = EvaluationDataset(goldens=[Golden(input="What is 3 * 12?")])
for golden in dataset.evals_iterator():
    task = asyncio.create_task(
        llm_with_tools.invoke(
            "What is 3 * 12?",
            config = {"callbacks": [CallbackHandler(metrics=[TaskCompletionMetric()])]}
        )
    )
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

âœ… Done. The `evals_iterator` will automatically generate a test run with individual evaluation traces for each golden.

</TimelineItem>

</Timeline>

:::note
If you need to evaluate individual components of your LangChain application, [set up tracing](/docs/evaluation-llm-tracing) instead.
:::

## Evals in Production

To run online evaluations in production, simply replace `metrics` in `CallbackHandler` with a [metric collection](https://documentation.confident-ai.com/docs/llm-tracing/evaluations#online-evaluations) string from Confident AI, and push your LangChain agent to production.

:::info
This will automatically evaluate all incoming traces in production with the task completion metrics defined in your [metric collection](https://documentation.confident-ai.com/docs/llm-tracing/evaluations#online-evaluations).
:::

```python filename="main.py" showLineNumbers {8}
from deepeval.integrations.langchain import CallbackHandler
...

# Invoke your agent with the metric collection name
llm_with_tools.invoke(
    "What is 3 * 12?",
    config = {"callbacks": [
        CallbackHandler(metric_collection="<metric-collection-name-with-task-completion>")
    ]}
)
```
