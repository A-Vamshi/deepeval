---
id: anthropic
title: Anthropic
sidebar_label: Anthropic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

`deepeval` provides integration with Anthropic's Claude models, enabling evaluation and tracing of your Anthropic applications through the `Anthropic` client wrapper. With deepeval, you can capture both end-to-end and component-level metrics, monitor quality in production and have insights into your Claude powered workflows.

## End-to-End Evals

Getting started with evaluating your Claude powered applications is easy, just swap out your existing `Anthropic` client for the `Anthropic` client provided by `deepeval`. Then, specify the `metrics` you want to track within the `trace` context manager to automatically capture evaluation data, insights and spans for your application.

<Tabs groupId="anthropic">
<TabItem value="messages" label="Messages">

```python title="main.py" showLineNumbers {4,15,16,17,18}
from deepeval.anthropic import Anthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

client = Anthropic()

goldens = [
    Golden(input="What is application of useState() in React?"),
    Golden(input="Compare Repeatable Read vs Read Committed as Isolation level for PostgreSQL."),
]
dataset = EvaluationDataset(goldens=goldens)

for golden in dataset.evals_iterator():
    with trace(
        metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ],
        )
```

</TabItem>
<TabItem value="async-messages" label="Async Messages">

```python title="main.py" showLineNumbers {6,11,12,13,14}
import asyncio

from deepeval.anthropic import AsyncAnthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

async_client = AsyncAnthropic()

async def run_claude(input):
    with trace(
        llm_metrics=[AnswerRelevancyMetric(), BiasMetric()],
        expected_output=golden.expected_output,
    ):
        async_client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ],
        )

goldens = [
    Golden(input="What is application of useState() in React?"),
    Golden(input="Compare Repeatable Read vs Read Committed as Isolation level for PostgreSQL."),
]
dataset = EvaluationDataset(goldens=goldens)

for golden in dataset.evals_iterator():
    task = asyncio.create_task(run_claude(input=golden.input))
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

There are **FIVE** optional parameters when using `deepeval`'s `Anthropic` client's messages method:

- [Optional] `metrics`: a list of metrics of type `BaseMetric`
- [Optional] `expected_output`: a string specifying the expected output of your Claude generation.
- [Optional] `retrieval_context`: a list of strings, representing the retrieved contexts to be passed into your Claude generation.
- [Optional] `context`: a list of strings, representing the ideal retrieved contexts to be passed into your Claude generation.
- [Optional] `expected_tools`: a list of strings, representing the expected tools to be called during Claude generation.

:::info
With `deepeval`'s `Anthropic` client, both the `input` and the `actual_output` are automatically extracted from each generation, so you can effortlessly evaluate your Claude based applications using metrics like **Answer Relevancy** without extra configuration. If you're using metrics that require additional context such as **Faithfulness**, which depends on retrieval context you can simply provide parameters like `retrieval_context` or `context` directly when making your API call to ensure accurate and meaningful evaluations.
:::

## Component-Level Evals

You can also use `deepeval`'s OpenAI client **within component-level evaluations**. To set up component-level evaluations, add the `@observe` decorator to your llm_application's components, and simply replace existing OpenAI clients with `deepeval`'s OpenAI client, passing in the metrics you wish to use.

<Tabs groupId="anthropic">
<TabItem value="messages" label="Messages">

```python title="main.py" showLineNumbers {4,6,13}
from deepeval.anthropic import Anthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, observe

@observe()
def retrieve_documents(query):
    return [
        "React is a popular Javascript library for building user interfaces.",
        "It allows developers to create large web applications that can update and render efficiently in response to data changes."
    ]

@observe()
def run_claude(input):
    client = Anthropic()
    with trace(llm_metrics=[AnswerRelevancyMetric()]):
        response = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ]
        )
    return response.content[0].text

dataset = EvaluationDataset(goldens=[Golden(input="...")])

for golden in dataset.evals_iterator():
    run_claude(input=golden.input)
```

</TabItem>
<TabItem value="async-messages" label="Async Messages">

```python title="main.py" showLineNumbers {6,8,15}
import asyncio

from deepeval.anthropic import AsyncAnthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, observe

@observe()
def retrieve_documents(query):
    return [
        "React is a popular Javascript library for building user interfaces.",
        "It allows developers to create large web applications that can update and render efficiently in response to data changes."
    ]

@observe()
def run_claude(input):
    client = AsyncAnthropic()
    with trace(llm_metrics=[AnswerRelevancyMetric()]):
        response = client.responses.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": input
                }
            ],
        )
    return response.content[0].text

dataset = EvaluationDataset(goldens=[Golden(input="...")])

for golden in dataset.evals_iterator():
    task = asyncio.create_task(run_claude(input=golden.input))
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

When used inside `@observe` components, `deepeval`'s `Anthropic` client automatically:

- Generates an LLM span for every Messages API call, including nested Tool spans for any tool invocations.
- Attaches an `LLMTestCase` to each generated LLM span, capturing inputs, outputs, and tools called.
- Records span level llm attributes such as the input prompt, generated output and token usage.
- Logs hyperparameters such as model name and system prompt for comprehensive experiment analysis.

## Online Evals in Production

If your Claude based application is in production, and you still want to run evaluations on your traces, use online evals. It lets you run evaluations on all incoming traces on Confident AI's server.

Set the `llm_metric_collection` name in the `trace` context when invoking your `Anthropic` client to evaluate LLM Spans.

```python main.py
from deepeval.anthropic import Anthropic
from deepeval.tracing import trace

client = Anthropic()

with trace(llm_metric_collection="test_collection_1"):
    response = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=4096,
        system="You are a helpful assistant.",
        messages=[
            {
                "role": "user",
                "content": "Hello, how are you?"
            }
        ],
    )
```
