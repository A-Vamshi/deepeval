---
id: anthropic
title: Anthropic
sidebar_label: Anthropic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

`deepeval` integrates with Anthropic models, allowing you to evaluate and trace Claude LLM requests, whether standalone or within complex applications with multiple components, in both development and production environment.

## Local Evaluations

Local (offline) evaluations run entirely on your machine using data points or datasets, this is the development environment.

### Evaluating Claude as a Standalone

Standalone evaluation treats your application as a black box, assessing its input and output using chosen metrics (e.g: `AnswerRelevancyMetric()`). To begin, simply swap out your existing `Anthropic` client with the one from `deepeval`.

<Tabs groupId="anthropic">
<TabItem value="messages" label="Messages">

```python title="main.py" showLineNumbers {4,12,13,14,15}
from deepeval.anthropic import Anthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

client = Anthropic()

dataset = EvaluationDataset()
datset.pull(alias="My Dataset")

for golden in dataset.evals_iterator():
    with trace(
        metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ],
        )
```

</TabItem>
<TabItem value="async-messages" label="Async Messages">

```python title="main.py" showLineNumbers {6,11,12,13,14}
import asyncio

from deepeval.anthropic import AsyncAnthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

async_client = AsyncAnthropic()

async def llm_app(input):
    with trace(
        llm_metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        await async_client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ],
        )

dataset = EvaluationDataset()
datset.pull(alias="My Dataset")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(llm_app(input=golden.input))
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

The `trace()` context supports **FIVE** optional parameters:

- `metrics`: List of `BaseMetric` metrics to use.
- `expected_output`: The reference output string for evaluation.
- `retrieval_context`: List of retrieved context strings for ground truth comparison.
- `context`: List of ideal context strings.
- `expected_tools`: List of expected tool names to be called during generation.

:::info
With `deepeval`â€™s `Anthropic` client, input and output are auto-extracted for every generation, so you can run evaluations like **Answer Relevancy** without extra setup. For metrics needing extra info (e.g. **Faithfulness**), just pass `retrieval_context` or `context` in the `trace()` context.
:::

### Evaluating Claude within Components

You can also use `deepeval`'s OpenAI client **within component-level evaluations**. To set up component-level evaluations, add the `@observe` decorator to your llm_application's components, and simply replace existing OpenAI clients with `deepeval`'s OpenAI client, passing in the metrics you wish to use.

<Tabs groupId="anthropic">
<TabItem value="messages" label="Messages">

```python title="main.py" showLineNumbers {4,6,13}
from deepeval.anthropic import Anthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, observe

@observe()
def retrieve_documents(query):
    return [
        "React is a popular Javascript library for building user interfaces.",
        "It allows developers to create large web applications that can update and render efficiently in response to data changes."
    ]

@observe()
def llm_app(input):
    client = Anthropic()
    with trace(llm_metrics=[AnswerRelevancyMetric()]):
        response = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": golden.input
                }
            ]
        )
    return response.content[0].text

dataset = EvaluationDataset()
datset.pull(alias="My Dataset")

for golden in dataset.evals_iterator():
    llm_app(input=golden.input)
```

</TabItem>
<TabItem value="async-messages" label="Async Messages">

```python title="main.py" showLineNumbers {6,8,15}
import asyncio

from deepeval.anthropic import AsyncAnthropic
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, observe

@observe()
def retrieve_documents(query):
    return [
        "React is a popular Javascript library for building user interfaces.",
        "It allows developers to create large web applications that can update and render efficiently in response to data changes."
    ]

@observe()
def llm_app(input):
    client = AsyncAnthropic()
    with trace(llm_metrics=[AnswerRelevancyMetric()]):
        response = client.responses.create(
            model="claude-sonnet-4-5",
            max_tokens=4096,
            system="You are a helpful assistant.",
            messages=[
                {
                    "role": "user",
                    "content": input
                }
            ],
        )
    return response.content[0].text

dataset = EvaluationDataset()
datset.pull(alias="My Dataset")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(llm_app(input=golden.input))
    dataset.evaluate(task)
```

</TabItem>
</Tabs>

When used inside `@observe` components, `deepeval`'s `Anthropic` client automatically:

- Generates an LLM span for every Messages API call, including nested Tool spans for any tool invocations.
- Attaches an `LLMTestCase` to each generated LLM span, capturing inputs, outputs, and tools called.
- Records span level llm attributes such as the input prompt, generated output and token usage.
- Logs hyperparameters such as model name and system prompt for comprehensive experiment analysis.

## Online Evals in Production

If your Claude based application is in production, and you still want to run evaluations on your traces, use online evals. It lets you run evaluations on all incoming traces on Confident AI's server.

Set the `llm_metric_collection` name in the `trace` context when invoking your `Anthropic` client to evaluate LLM Spans.

```python main.py
from deepeval.anthropic import Anthropic
from deepeval.tracing import trace

client = Anthropic()

with trace(llm_metric_collection="test_collection_1"):
    response = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=4096,
        system="You are a helpful assistant.",
        messages=[
            {
                "role": "user",
                "content": "Hello, how are you?"
            }
        ],
    )
```

:::note
For a complete guide on setting up online evaluations with **Confident AI** (the `deepeval` cloud platform), please visit [Evaluating with Tracing](/docs/evaluation-component-level-llm-evals).
:::
